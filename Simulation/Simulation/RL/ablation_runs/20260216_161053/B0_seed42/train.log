Using cpu device
Debut de l'entrainement...
Logging to /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/logs/tensorboard/PPO_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 283      |
|    ep_rew_mean     | -19.7    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 1793     |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=16.56 +/- 0.00
Episode length: 544.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 544         |
|    mean_reward          | 16.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.006569122 |
|    clip_fraction        | 0.064       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00797     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.005      |
|    std                  | 0.996       |
|    value_loss           | 0.0884      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 332      |
|    ep_rew_mean     | -20.1    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 965      |
|    iterations      | 2        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=15.74 +/- 0.00
Episode length: 459.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 459         |
|    mean_reward          | 15.7        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.009185689 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.84       |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0209     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00919    |
|    std                  | 1           |
|    value_loss           | 0.00709     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 364      |
|    ep_rew_mean     | -18.7    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 806      |
|    iterations      | 3        |
|    time_elapsed    | 30       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=30000, episode_reward=23.03 +/- 0.00
Episode length: 533.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 533         |
|    mean_reward          | 23          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.012084686 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.229       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0247     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.016      |
|    std                  | 0.992       |
|    value_loss           | 0.00813     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 387      |
|    ep_rew_mean     | -16.4    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 741      |
|    iterations      | 4        |
|    time_elapsed    | 44       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=40000, episode_reward=38.56 +/- 0.00
Episode length: 777.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 777         |
|    mean_reward          | 38.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.014956281 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | -0.194      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0179     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.014      |
|    std                  | 0.974       |
|    value_loss           | 0.00623     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | -15      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 720      |
|    iterations      | 5        |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 480         |
|    ep_rew_mean          | -14.9       |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 713         |
|    iterations           | 6           |
|    time_elapsed         | 68          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.014165342 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00615    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.013      |
|    std                  | 0.969       |
|    value_loss           | 0.00534     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=81.87 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 81.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010987025 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0347     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0143     |
|    std                  | 0.976       |
|    value_loss           | 0.00557     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 544      |
|    ep_rew_mean     | -14.2    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 669      |
|    iterations      | 7        |
|    time_elapsed    | 85       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=14.12 +/- 0.00
Episode length: 337.00 +/- 0.00
Success rate: 0.00%
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 337      |
|    mean_reward          | 14.1     |
|    success_rate         | 0        |
| time/                   |          |
|    total_timesteps      | 60000    |
| train/                  |          |
|    approx_kl            | 0.017975 |
|    clip_fraction        | 0.208    |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.78    |
|    explained_variance   | 0.875    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0039   |
|    n_updates            | 70       |
|    policy_gradient_loss | -0.0231  |
|    std                  | 0.987    |
|    value_loss           | 0.00313  |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 614      |
|    ep_rew_mean     | -12.9    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 678      |
|    iterations      | 8        |
|    time_elapsed    | 96       |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=70000, episode_reward=10.74 +/- 0.00
Episode length: 298.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 298         |
|    mean_reward          | 10.7        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.014241463 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0371     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0158     |
|    std                  | 1.01        |
|    value_loss           | 0.0114      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 674      |
|    ep_rew_mean     | -9.49    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 685      |
|    iterations      | 9        |
|    time_elapsed    | 107      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=80000, episode_reward=6.32 +/- 0.00
Episode length: 278.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 278         |
|    mean_reward          | 6.32        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.011139451 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00285    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0107     |
|    std                  | 1.01        |
|    value_loss           | 0.0137      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 729      |
|    ep_rew_mean     | -4.27    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 681      |
|    iterations      | 10       |
|    time_elapsed    | 120      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=90000, episode_reward=7.37 +/- 0.00
Episode length: 213.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 213         |
|    mean_reward          | 7.37        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.011230892 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.79       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00629    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0103     |
|    std                  | 0.997       |
|    value_loss           | 0.0116      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 558      |
|    ep_rew_mean     | 6.24     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 688      |
|    iterations      | 11       |
|    time_elapsed    | 130      |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 140         |
|    ep_rew_mean          | 4.98        |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 697         |
|    iterations           | 12          |
|    time_elapsed         | 140         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.009549134 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.78       |
|    explained_variance   | -0.118      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00544     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00722    |
|    std                  | 0.994       |
|    value_loss           | 0.0136      |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=9.49 +/- 0.00
Episode length: 168.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 168         |
|    mean_reward          | 9.49        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.013493503 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.76       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00674    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0134     |
|    std                  | 0.995       |
|    value_loss           | 0.00912     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 5.99     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 703      |
|    iterations      | 13       |
|    time_elapsed    | 151      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=110000, episode_reward=10.96 +/- 0.00
Episode length: 165.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | 11          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.013393741 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0357     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.016      |
|    std                  | 0.969       |
|    value_loss           | 0.00921     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 7.86     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 709      |
|    iterations      | 14       |
|    time_elapsed    | 161      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=120000, episode_reward=103.44 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 103         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.013356587 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.68       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00545     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0166     |
|    std                  | 0.957       |
|    value_loss           | 0.00946     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 10.4     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 682      |
|    iterations      | 15       |
|    time_elapsed    | 180      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=130000, episode_reward=147.60 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 148         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.015050111 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0076     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0207     |
|    std                  | 0.95        |
|    value_loss           | 0.00846     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 14.4     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 660      |
|    iterations      | 16       |
|    time_elapsed    | 198      |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 354         |
|    ep_rew_mean          | 18.2        |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 661         |
|    iterations           | 17          |
|    time_elapsed         | 210         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.018030602 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.64       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0148      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0218     |
|    std                  | 0.946       |
|    value_loss           | 0.0101      |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=175.37 +/- 0.00
Episode length: 1890.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.89e+03    |
|    mean_reward          | 175         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.016148236 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00856    |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0224     |
|    std                  | 0.938       |
|    value_loss           | 0.00912     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | 27.2     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 643      |
|    iterations      | 18       |
|    time_elapsed    | 229      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=186.51 +/- 0.00
Episode length: 1801.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.8e+03     |
|    mean_reward          | 187         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.017072603 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00875    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0226     |
|    std                  | 0.931       |
|    value_loss           | 0.0127      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 519      |
|    ep_rew_mean     | 32.9     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 629      |
|    iterations      | 19       |
|    time_elapsed    | 247      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=160000, episode_reward=195.75 +/- 0.00
Episode length: 1628.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.63e+03    |
|    mean_reward          | 196         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.018432865 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0147     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0248     |
|    std                  | 0.908       |
|    value_loss           | 0.0118      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 589      |
|    ep_rew_mean     | 40.6     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 619      |
|    iterations      | 20       |
|    time_elapsed    | 264      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=170000, episode_reward=205.56 +/- 0.00
Episode length: 1539.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.54e+03    |
|    mean_reward          | 206         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.017612945 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00799    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0247     |
|    std                  | 0.89        |
|    value_loss           | 0.00894     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 669      |
|    ep_rew_mean     | 50.5     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 615      |
|    iterations      | 21       |
|    time_elapsed    | 279      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=180000, episode_reward=218.48 +/- 0.00
Episode length: 1528.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.53e+03   |
|    mean_reward          | 218        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 180000     |
| train/                  |            |
|    approx_kl            | 0.01863233 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.49      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.061     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0255    |
|    std                  | 0.873      |
|    value_loss           | 0.00804    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 746      |
|    ep_rew_mean     | 61.1     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 607      |
|    iterations      | 22       |
|    time_elapsed    | 296      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 819         |
|    ep_rew_mean          | 72.3        |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 611         |
|    iterations           | 23          |
|    time_elapsed         | 308         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.017393196 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00869     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0264     |
|    std                  | 0.842       |
|    value_loss           | 0.00492     |
-----------------------------------------
Eval num_timesteps=190000, episode_reward=243.92 +/- 0.00
Episode length: 1621.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.62e+03    |
|    mean_reward          | 244         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 0.017027661 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.011      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0245     |
|    std                  | 0.82        |
|    value_loss           | 0.00336     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 892      |
|    ep_rew_mean     | 82.6     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 602      |
|    iterations      | 24       |
|    time_elapsed    | 326      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=267.16 +/- 0.00
Episode length: 1932.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.93e+03    |
|    mean_reward          | 267         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.014455546 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0178     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0161     |
|    std                  | 0.795       |
|    value_loss           | 0.00324     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 95.3     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 595      |
|    iterations      | 25       |
|    time_elapsed    | 344      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=210000, episode_reward=273.89 +/- 0.00
Episode length: 1716.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.72e+03    |
|    mean_reward          | 274         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.016263716 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0333     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0232     |
|    std                  | 0.766       |
|    value_loss           | 0.00216     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.04e+03 |
|    ep_rew_mean     | 104      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 589      |
|    iterations      | 26       |
|    time_elapsed    | 361      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=220000, episode_reward=276.96 +/- 0.00
Episode length: 1516.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.52e+03    |
|    mean_reward          | 277         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.015597727 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.026      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0219     |
|    std                  | 0.747       |
|    value_loss           | 0.00221     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.1e+03  |
|    ep_rew_mean     | 118      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 588      |
|    iterations      | 27       |
|    time_elapsed    | 375      |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.18e+03    |
|    ep_rew_mean          | 133         |
|    success_rate         | 0.02        |
| time/                   |             |
|    fps                  | 594         |
|    iterations           | 28          |
|    time_elapsed         | 385         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.015716597 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0157      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0189     |
|    std                  | 0.732       |
|    value_loss           | 0.00214     |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=281.99 +/- 0.00
Episode length: 1275.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.28e+03    |
|    mean_reward          | 282         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.017413436 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0257     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0202     |
|    std                  | 0.716       |
|    value_loss           | 0.00194     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | 150      |
|    success_rate    | 0.07     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 29       |
|    time_elapsed    | 399      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=240000, episode_reward=283.59 +/- 0.00
Episode length: 1218.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.22e+03    |
|    mean_reward          | 284         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.017160058 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.013       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0167     |
|    std                  | 0.702       |
|    value_loss           | 0.00146     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.3e+03  |
|    ep_rew_mean     | 165      |
|    success_rate    | 0.13     |
| time/              |          |
|    fps             | 591      |
|    iterations      | 30       |
|    time_elapsed    | 415      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=250000, episode_reward=283.09 +/- 0.00
Episode length: 1127.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | 283         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.015936509 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0178     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0155     |
|    std                  | 0.688       |
|    value_loss           | 0.00108     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.31e+03 |
|    ep_rew_mean     | 182      |
|    success_rate    | 0.21     |
| time/              |          |
|    fps             | 588      |
|    iterations      | 31       |
|    time_elapsed    | 431      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=260000, episode_reward=283.78 +/- 0.00
Episode length: 1115.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.12e+03    |
|    mean_reward          | 284         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 260000      |
| train/                  |             |
|    approx_kl            | 0.016103141 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0146     |
|    std                  | 0.673       |
|    value_loss           | 0.00109     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.29e+03 |
|    ep_rew_mean     | 195      |
|    success_rate    | 0.28     |
| time/              |          |
|    fps             | 586      |
|    iterations      | 32       |
|    time_elapsed    | 447      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=270000, episode_reward=284.52 +/- 0.00
Episode length: 1050.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.05e+03   |
|    mean_reward          | 285        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 270000     |
| train/                  |            |
|    approx_kl            | 0.01616945 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.9       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0293    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0153    |
|    std                  | 0.654      |
|    value_loss           | 0.000771   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.27e+03 |
|    ep_rew_mean     | 209      |
|    success_rate    | 0.36     |
| time/              |          |
|    fps             | 584      |
|    iterations      | 33       |
|    time_elapsed    | 462      |
|    total_timesteps | 270336   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 224         |
|    success_rate         | 0.44        |
| time/                   |             |
|    fps                  | 588         |
|    iterations           | 34          |
|    time_elapsed         | 473         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.014602459 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00138    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0104     |
|    std                  | 0.65        |
|    value_loss           | 0.0139      |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=285.22 +/- 0.00
Episode length: 958.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 958         |
|    mean_reward          | 285         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.013233906 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00946     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0131     |
|    std                  | 0.645       |
|    value_loss           | 0.000789    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | 236      |
|    success_rate    | 0.52     |
| time/              |          |
|    fps             | 586      |
|    iterations      | 35       |
|    time_elapsed    | 488      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=290000, episode_reward=284.80 +/- 0.00
Episode length: 940.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 940         |
|    mean_reward          | 285         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.012964052 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0088      |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0125     |
|    std                  | 0.631       |
|    value_loss           | 0.000611    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.22e+03 |
|    ep_rew_mean     | 247      |
|    success_rate    | 0.61     |
| time/              |          |
|    fps             | 585      |
|    iterations      | 36       |
|    time_elapsed    | 503      |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=300000, episode_reward=285.20 +/- 0.00
Episode length: 915.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 915         |
|    mean_reward          | 285         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.012503999 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00786    |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00898    |
|    std                  | 0.616       |
|    value_loss           | 0.000529    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.19e+03 |
|    ep_rew_mean     | 254      |
|    success_rate    | 0.69     |
| time/              |          |
|    fps             | 585      |
|    iterations      | 37       |
|    time_elapsed    | 518      |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=310000, episode_reward=285.18 +/- 0.00
Episode length: 903.00 +/- 0.00
Success rate: 100.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 903          |
|    mean_reward          | 285          |
|    success_rate         | 1            |
| time/                   |              |
|    total_timesteps      | 310000       |
| train/                  |              |
|    approx_kl            | 0.0118822325 |
|    clip_fraction        | 0.158        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0204      |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00827     |
|    std                  | 0.598        |
|    value_loss           | 0.000454     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | 261      |
|    success_rate    | 0.79     |
| time/              |          |
|    fps             | 584      |
|    iterations      | 38       |
|    time_elapsed    | 532      |
|    total_timesteps | 311296   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.07e+03    |
|    ep_rew_mean          | 264         |
|    success_rate         | 0.87        |
| time/                   |             |
|    fps                  | 586         |
|    iterations           | 39          |
|    time_elapsed         | 545         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.014266161 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00731     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0113     |
|    std                  | 0.576       |
|    value_loss           | 0.00038     |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=285.38 +/- 0.00
Episode length: 883.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 883         |
|    mean_reward          | 285         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.014710031 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00387    |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0108     |
|    std                  | 0.559       |
|    value_loss           | 0.00108     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.03e+03 |
|    ep_rew_mean     | 267      |
|    success_rate    | 0.93     |
| time/              |          |
|    fps             | 585      |
|    iterations      | 40       |
|    time_elapsed    | 559      |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=330000, episode_reward=286.07 +/- 0.00
Episode length: 866.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 866         |
|    mean_reward          | 286         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.012220627 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.02       |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00756    |
|    std                  | 0.543       |
|    value_loss           | 0.000266    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 991      |
|    ep_rew_mean     | 269      |
|    success_rate    | 0.98     |
| time/              |          |
|    fps             | 587      |
|    iterations      | 41       |
|    time_elapsed    | 571      |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=340000, episode_reward=285.65 +/- 0.00
Episode length: 850.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 850         |
|    mean_reward          | 286         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.011952167 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0153     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00858    |
|    std                  | 0.527       |
|    value_loss           | 0.000227    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 270      |
|    success_rate    | 0.98     |
| time/              |          |
|    fps             | 588      |
|    iterations      | 42       |
|    time_elapsed    | 584      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=350000, episode_reward=285.93 +/- 0.00
Episode length: 842.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 842         |
|    mean_reward          | 286         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.014169324 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0307     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00896    |
|    std                  | 0.512       |
|    value_loss           | 0.000243    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 933      |
|    ep_rew_mean     | 272      |
|    success_rate    | 0.98     |
| time/              |          |
|    fps             | 588      |
|    iterations      | 43       |
|    time_elapsed    | 598      |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=360000, episode_reward=285.37 +/- 0.00
Episode length: 832.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 832         |
|    mean_reward          | 285         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.014405647 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0152     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00694    |
|    std                  | 0.496       |
|    value_loss           | 0.000162    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 908      |
|    ep_rew_mean     | 273      |
|    success_rate    | 0.97     |
| time/              |          |
|    fps             | 587      |
|    iterations      | 44       |
|    time_elapsed    | 613      |
|    total_timesteps | 360448   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 836         |
|    ep_rew_mean          | 258         |
|    success_rate         | 0.91        |
| time/                   |             |
|    fps                  | 589         |
|    iterations           | 45          |
|    time_elapsed         | 625         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.011167304 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00724     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00358    |
|    std                  | 0.484       |
|    value_loss           | 0.0122      |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=285.94 +/- 0.00
Episode length: 828.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 828         |
|    mean_reward          | 286         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.013358856 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0395      |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00312    |
|    std                  | 0.502       |
|    value_loss           | 0.0442      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 797      |
|    ep_rew_mean     | 251      |
|    success_rate    | 0.88     |
| time/              |          |
|    fps             | 588      |
|    iterations      | 46       |
|    time_elapsed    | 639      |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=380000, episode_reward=284.99 +/- 0.00
Episode length: 827.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 827         |
|    mean_reward          | 285         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 380000      |
| train/                  |             |
|    approx_kl            | 0.009740537 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00243    |
|    std                  | 0.499       |
|    value_loss           | 0.0222      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 788      |
|    ep_rew_mean     | 251      |
|    success_rate    | 0.88     |
| time/              |          |
|    fps             | 588      |
|    iterations      | 47       |
|    time_elapsed    | 654      |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=390000, episode_reward=285.13 +/- 0.00
Episode length: 819.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 819         |
|    mean_reward          | 285         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.013537813 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0187      |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00184    |
|    std                  | 0.482       |
|    value_loss           | 0.000759    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 772      |
|    ep_rew_mean     | 249      |
|    success_rate    | 0.87     |
| time/              |          |
|    fps             | 587      |
|    iterations      | 48       |
|    time_elapsed    | 668      |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=400000, episode_reward=285.83 +/- 0.00
Episode length: 813.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 813         |
|    mean_reward          | 286         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.010963916 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0389     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00129    |
|    std                  | 0.469       |
|    value_loss           | 0.00946     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 763      |
|    ep_rew_mean     | 250      |
|    success_rate    | 0.87     |
| time/              |          |
|    fps             | 587      |
|    iterations      | 49       |
|    time_elapsed    | 683      |
|    total_timesteps | 401408   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 753         |
|    ep_rew_mean          | 249         |
|    success_rate         | 0.83        |
| time/                   |             |
|    fps                  | 589         |
|    iterations           | 50          |
|    time_elapsed         | 695         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.012269648 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00492     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00358    |
|    std                  | 0.449       |
|    value_loss           | 0.00105     |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=285.17 +/- 0.00
Episode length: 812.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 812         |
|    mean_reward          | 285         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.011298314 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0225      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00117    |
|    std                  | 0.445       |
|    value_loss           | 0.00247     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 745      |
|    ep_rew_mean     | 249      |
|    success_rate    | 0.79     |
| time/              |          |
|    fps             | 590      |
|    iterations      | 51       |
|    time_elapsed    | 707      |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=420000, episode_reward=269.78 +/- 0.00
Episode length: 739.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 739          |
|    mean_reward          | 270          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0112771625 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00468     |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00154     |
|    std                  | 0.436        |
|    value_loss           | 0.0022       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 736      |
|    ep_rew_mean     | 248      |
|    success_rate    | 0.74     |
| time/              |          |
|    fps             | 592      |
|    iterations      | 52       |
|    time_elapsed    | 719      |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=430000, episode_reward=286.71 +/- 0.00
Episode length: 806.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 806         |
|    mean_reward          | 287         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.011760409 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0551      |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00139    |
|    std                  | 0.431       |
|    value_loss           | 0.00217     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 738      |
|    ep_rew_mean     | 250      |
|    success_rate    | 0.71     |
| time/              |          |
|    fps             | 591      |
|    iterations      | 53       |
|    time_elapsed    | 733      |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=440000, episode_reward=286.36 +/- 0.00
Episode length: 802.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 802         |
|    mean_reward          | 286         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.014171927 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0143     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00384    |
|    std                  | 0.42        |
|    value_loss           | 0.00165     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 767      |
|    ep_rew_mean     | 261      |
|    success_rate    | 0.75     |
| time/              |          |
|    fps             | 592      |
|    iterations      | 54       |
|    time_elapsed    | 746      |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=450000, episode_reward=287.60 +/- 0.00
Episode length: 807.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 807         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.014415385 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00814    |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00338    |
|    std                  | 0.408       |
|    value_loss           | 0.000295    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 789      |
|    ep_rew_mean     | 269      |
|    success_rate    | 0.78     |
| time/              |          |
|    fps             | 592      |
|    iterations      | 55       |
|    time_elapsed    | 760      |
|    total_timesteps | 450560   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 802         |
|    ep_rew_mean          | 275         |
|    success_rate         | 0.8         |
| time/                   |             |
|    fps                  | 593         |
|    iterations           | 56          |
|    time_elapsed         | 772         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.017094174 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.96       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00458    |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00687    |
|    std                  | 0.392       |
|    value_loss           | 0.000138    |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=288.42 +/- 0.00
Episode length: 810.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 810         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.016143223 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.889      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000937    |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00851    |
|    std                  | 0.38        |
|    value_loss           | 0.00012     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 800      |
|    ep_rew_mean     | 275      |
|    success_rate    | 0.8      |
| time/              |          |
|    fps             | 593      |
|    iterations      | 57       |
|    time_elapsed    | 786      |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=470000, episode_reward=288.26 +/- 0.00
Episode length: 807.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 807         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.014369642 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.856      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00575    |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00506    |
|    std                  | 0.374       |
|    value_loss           | 0.000121    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 806      |
|    ep_rew_mean     | 278      |
|    success_rate    | 0.81     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 58       |
|    time_elapsed    | 799      |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=480000, episode_reward=288.15 +/- 0.00
Episode length: 796.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 796         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.016664263 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.806      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00394    |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00863    |
|    std                  | 0.364       |
|    value_loss           | 9.91e-05    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 805      |
|    ep_rew_mean     | 279      |
|    success_rate    | 0.82     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 59       |
|    time_elapsed    | 813      |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=490000, episode_reward=288.51 +/- 0.00
Episode length: 793.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 793         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.014691131 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.756      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00969     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0049     |
|    std                  | 0.355       |
|    value_loss           | 0.000115    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 806      |
|    ep_rew_mean     | 280      |
|    success_rate    | 0.86     |
| time/              |          |
|    fps             | 593      |
|    iterations      | 60       |
|    time_elapsed    | 827      |
|    total_timesteps | 491520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 806        |
|    ep_rew_mean          | 281        |
|    success_rate         | 0.9        |
| time/                   |            |
|    fps                  | 596        |
|    iterations           | 61         |
|    time_elapsed         | 837        |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.01685751 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.686     |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00421   |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0076    |
|    std                  | 0.342      |
|    value_loss           | 7.92e-05   |
----------------------------------------
Eval num_timesteps=500000, episode_reward=288.49 +/- 0.00
Episode length: 788.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 788         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.014403012 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.635      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0104     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00584    |
|    std                  | 0.334       |
|    value_loss           | 9.96e-05    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 807      |
|    ep_rew_mean     | 282      |
|    success_rate    | 0.95     |
| time/              |          |
|    fps             | 597      |
|    iterations      | 62       |
|    time_elapsed    | 849      |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=510000, episode_reward=288.37 +/- 0.00
Episode length: 785.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 785         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.014861474 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.576      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0314     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00312    |
|    std                  | 0.322       |
|    value_loss           | 7.03e-05    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 800      |
|    ep_rew_mean     | 281      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 599      |
|    iterations      | 63       |
|    time_elapsed    | 861      |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=520000, episode_reward=288.31 +/- 0.00
Episode length: 784.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 784         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 520000      |
| train/                  |             |
|    approx_kl            | 0.013455666 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0423      |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00277    |
|    std                  | 0.312       |
|    value_loss           | 0.00838     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 796      |
|    ep_rew_mean     | 281      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 600      |
|    iterations      | 64       |
|    time_elapsed    | 873      |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=530000, episode_reward=288.53 +/- 0.00
Episode length: 781.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 781        |
|    mean_reward          | 289        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 530000     |
| train/                  |            |
|    approx_kl            | 0.01214328 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.465     |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00558    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.000585  |
|    std                  | 0.305      |
|    value_loss           | 0.000264   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 792      |
|    ep_rew_mean     | 282      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 600      |
|    iterations      | 65       |
|    time_elapsed    | 887      |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=540000, episode_reward=288.25 +/- 0.00
Episode length: 777.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 777         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 540000      |
| train/                  |             |
|    approx_kl            | 0.014516629 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.427      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0178      |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00242    |
|    std                  | 0.299       |
|    value_loss           | 6.6e-05     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 789      |
|    ep_rew_mean     | 282      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 599      |
|    iterations      | 66       |
|    time_elapsed    | 901      |
|    total_timesteps | 540672   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 776         |
|    ep_rew_mean          | 279         |
|    success_rate         | 0.98        |
| time/                   |             |
|    fps                  | 600         |
|    iterations           | 67          |
|    time_elapsed         | 913         |
|    total_timesteps      | 548864      |
| train/                  |             |
|    approx_kl            | 0.015190567 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.373      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0128     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00336    |
|    std                  | 0.291       |
|    value_loss           | 5.94e-05    |
-----------------------------------------
Eval num_timesteps=550000, episode_reward=288.88 +/- 0.00
Episode length: 772.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 772         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.014286092 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.326      |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0065      |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00026    |
|    std                  | 0.285       |
|    value_loss           | 0.00858     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 772      |
|    ep_rew_mean     | 280      |
|    success_rate    | 0.98     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 68       |
|    time_elapsed    | 925      |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=560000, episode_reward=289.44 +/- 0.00
Episode length: 768.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 768         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.019176148 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.297      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0308     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.000869   |
|    std                  | 0.281       |
|    value_loss           | 0.000302    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 769      |
|    ep_rew_mean     | 280      |
|    success_rate    | 0.98     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 69       |
|    time_elapsed    | 939      |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=570000, episode_reward=289.17 +/- 0.00
Episode length: 768.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 768         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 570000      |
| train/                  |             |
|    approx_kl            | 0.014605086 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.241      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00835    |
|    n_updates            | 690         |
|    policy_gradient_loss | -7.66e-05   |
|    std                  | 0.273       |
|    value_loss           | 7.49e-05    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 766      |
|    ep_rew_mean     | 280      |
|    success_rate    | 0.98     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 70       |
|    time_elapsed    | 953      |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=580000, episode_reward=289.04 +/- 0.00
Episode length: 768.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 768         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.016008798 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0261     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00167    |
|    std                  | 0.267       |
|    value_loss           | 5.84e-05    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 764      |
|    ep_rew_mean     | 280      |
|    success_rate    | 0.98     |
| time/              |          |
|    fps             | 602      |
|    iterations      | 71       |
|    time_elapsed    | 965      |
|    total_timesteps | 581632   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 770         |
|    ep_rew_mean          | 283         |
|    success_rate         | 0.99        |
| time/                   |             |
|    fps                  | 604         |
|    iterations           | 72          |
|    time_elapsed         | 975         |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.016389111 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0125      |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00256    |
|    std                  | 0.257       |
|    value_loss           | 6.75e-05    |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=289.44 +/- 0.00
Episode length: 763.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 763         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.014965776 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0857     |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.017       |
|    n_updates            | 720         |
|    policy_gradient_loss | 0.000133    |
|    std                  | 0.253       |
|    value_loss           | 5.19e-05    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 767      |
|    ep_rew_mean     | 283      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 604      |
|    iterations      | 73       |
|    time_elapsed    | 989      |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=600000, episode_reward=289.33 +/- 0.00
Episode length: 764.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 764         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.013640356 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0526     |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0264      |
|    n_updates            | 730         |
|    policy_gradient_loss | -6.42e-05   |
|    std                  | 0.248       |
|    value_loss           | 3.97e-05    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 765      |
|    ep_rew_mean     | 284      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 74       |
|    time_elapsed    | 1001     |
|    total_timesteps | 606208   |
---------------------------------
Modele final sauvegarde: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/final_model.zip
Meilleur modele sauvegarde: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/best_model.zip
Statistiques VecNormalize: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/vecnormalize.pkl
