Using cpu device
Debut de l'entrainement...
Logging to /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/logs/tensorboard/PPO_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 265      |
|    ep_rew_mean     | -14.5    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 1793     |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-15.27 +/- 0.00
Episode length: 255.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | -15.3       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.006459061 |
|    clip_fraction        | 0.0616      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0251      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00456    |
|    std                  | 0.985       |
|    value_loss           | 0.126       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 284      |
|    ep_rew_mean     | -17.1    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 1029     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.71 +/- 0.00
Episode length: 236.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 236         |
|    mean_reward          | -0.708      |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.009157067 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.79       |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00274    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0106     |
|    std                  | 0.971       |
|    value_loss           | 0.0143      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 361      |
|    ep_rew_mean     | -21.8    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 856      |
|    iterations      | 3        |
|    time_elapsed    | 28       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=30000, episode_reward=1.12 +/- 0.00
Episode length: 231.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 231         |
|    mean_reward          | 1.12        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.011373961 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.78       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00835    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0137     |
|    std                  | 0.971       |
|    value_loss           | 0.00508     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | -15.2    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 828      |
|    iterations      | 4        |
|    time_elapsed    | 39       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=40000, episode_reward=4.27 +/- 0.00
Episode length: 232.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 232         |
|    mean_reward          | 4.27        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.017313577 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0563     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0283     |
|    std                  | 0.964       |
|    value_loss           | 0.0071      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | -3.26    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 784      |
|    iterations      | 5        |
|    time_elapsed    | 52       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 152         |
|    ep_rew_mean          | -1.09       |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 767         |
|    iterations           | 6           |
|    time_elapsed         | 64          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.020976922 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.74       |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.07       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0328     |
|    std                  | 0.956       |
|    value_loss           | 0.00453     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=7.33 +/- 0.00
Episode length: 250.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 250        |
|    mean_reward          | 7.33       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.01954208 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.74      |
|    explained_variance   | 0.285      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0166    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0281    |
|    std                  | 0.965      |
|    value_loss           | 0.00465    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 0.52     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 765      |
|    iterations      | 7        |
|    time_elapsed    | 74       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=8.46 +/- 0.00
Episode length: 261.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 261         |
|    mean_reward          | 8.46        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.015283385 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.74       |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00866    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0246     |
|    std                  | 0.965       |
|    value_loss           | 0.0045      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 1.89     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 765      |
|    iterations      | 8        |
|    time_elapsed    | 85       |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=70000, episode_reward=10.06 +/- 0.00
Episode length: 237.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 237         |
|    mean_reward          | 10.1        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.014542633 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.468       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0205     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0255     |
|    std                  | 0.956       |
|    value_loss           | 0.00365     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 3.15     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 763      |
|    iterations      | 9        |
|    time_elapsed    | 96       |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=80000, episode_reward=12.69 +/- 0.00
Episode length: 210.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 12.7       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.01587138 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.7       |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0412    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0254    |
|    std                  | 0.948      |
|    value_loss           | 0.00438    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 3.47     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 760      |
|    iterations      | 10       |
|    time_elapsed    | 107      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=90000, episode_reward=32.39 +/- 0.00
Episode length: 1506.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.51e+03    |
|    mean_reward          | 32.4        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.011853879 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.69       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0115     |
|    std                  | 0.947       |
|    value_loss           | 0.00799     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 2.96     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 731      |
|    iterations      | 11       |
|    time_elapsed    | 123      |
|    total_timesteps | 90112    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 293        |
|    ep_rew_mean          | 1.34       |
|    success_rate         | 0          |
| time/                   |            |
|    fps                  | 726        |
|    iterations           | 12         |
|    time_elapsed         | 135        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.01064406 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.67      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00453    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00889   |
|    std                  | 0.937      |
|    value_loss           | 0.00898    |
----------------------------------------
Eval num_timesteps=100000, episode_reward=38.57 +/- 0.00
Episode length: 1314.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.31e+03    |
|    mean_reward          | 38.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.010301302 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.64       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0105     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0123     |
|    std                  | 0.924       |
|    value_loss           | 0.0115      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 357      |
|    ep_rew_mean     | 0.775    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 701      |
|    iterations      | 13       |
|    time_elapsed    | 151      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=110000, episode_reward=48.33 +/- 0.00
Episode length: 1204.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.2e+03     |
|    mean_reward          | 48.3        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.014839362 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.018      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.017      |
|    std                  | 0.905       |
|    value_loss           | 0.0119      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 443      |
|    ep_rew_mean     | 0.913    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 682      |
|    iterations      | 14       |
|    time_elapsed    | 168      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=120000, episode_reward=50.83 +/- 0.00
Episode length: 1191.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.19e+03    |
|    mean_reward          | 50.8        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.016003689 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0264     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0194     |
|    std                  | 0.878       |
|    value_loss           | 0.00904     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | 1.76     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 669      |
|    iterations      | 15       |
|    time_elapsed    | 183      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=130000, episode_reward=53.50 +/- 0.00
Episode length: 1189.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.19e+03    |
|    mean_reward          | 53.5        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.017393332 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0312     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0216     |
|    std                  | 0.863       |
|    value_loss           | 0.00667     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 580      |
|    ep_rew_mean     | 2.6      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 665      |
|    iterations      | 16       |
|    time_elapsed    | 196      |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 646         |
|    ep_rew_mean          | 4.55        |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 673         |
|    iterations           | 17          |
|    time_elapsed         | 206         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.016704408 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.44       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.021      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0185     |
|    std                  | 0.839       |
|    value_loss           | 0.00646     |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=55.86 +/- 0.00
Episode length: 1118.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.12e+03    |
|    mean_reward          | 55.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.015663572 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0207     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0208     |
|    std                  | 0.821       |
|    value_loss           | 0.00347     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 730      |
|    ep_rew_mean     | 7.72     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 669      |
|    iterations      | 18       |
|    time_elapsed    | 220      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=56.11 +/- 0.00
Episode length: 1095.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.1e+03     |
|    mean_reward          | 56.1        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.016941842 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0201     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0192     |
|    std                  | 0.797       |
|    value_loss           | 0.00201     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 798      |
|    ep_rew_mean     | 10.6     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 666      |
|    iterations      | 19       |
|    time_elapsed    | 233      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=160000, episode_reward=55.64 +/- 0.00
Episode length: 1041.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.04e+03    |
|    mean_reward          | 55.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.014435362 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0302     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0154     |
|    std                  | 0.784       |
|    value_loss           | 0.00137     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 856      |
|    ep_rew_mean     | 13.4     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 664      |
|    iterations      | 20       |
|    time_elapsed    | 246      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=170000, episode_reward=57.00 +/- 0.00
Episode length: 994.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 994         |
|    mean_reward          | 57          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.014439885 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0187     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0141     |
|    std                  | 0.777       |
|    value_loss           | 0.00611     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 920      |
|    ep_rew_mean     | 17.7     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 658      |
|    iterations      | 21       |
|    time_elapsed    | 261      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=180000, episode_reward=58.66 +/- 0.00
Episode length: 964.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 964         |
|    mean_reward          | 58.7        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.016202707 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0104     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0181     |
|    std                  | 0.761       |
|    value_loss           | 0.00126     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 948      |
|    ep_rew_mean     | 21.9     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 652      |
|    iterations      | 22       |
|    time_elapsed    | 276      |
|    total_timesteps | 180224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 934        |
|    ep_rew_mean          | 28.7       |
|    success_rate         | 0          |
| time/                   |            |
|    fps                  | 653        |
|    iterations           | 23         |
|    time_elapsed         | 288        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.01641996 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.1       |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0255    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0181    |
|    std                  | 0.743      |
|    value_loss           | 0.00784    |
----------------------------------------
Eval num_timesteps=190000, episode_reward=64.76 +/- 0.00
Episode length: 877.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 877         |
|    mean_reward          | 64.8        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 0.017947067 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00176     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0178     |
|    std                  | 0.729       |
|    value_loss           | 0.00112     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 883      |
|    ep_rew_mean     | 34.3     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 649      |
|    iterations      | 24       |
|    time_elapsed    | 302      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=67.48 +/- 0.00
Episode length: 821.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 821         |
|    mean_reward          | 67.5        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.016314283 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00681    |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0166     |
|    std                  | 0.713       |
|    value_loss           | 0.00904     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 849      |
|    ep_rew_mean     | 39.6     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 650      |
|    iterations      | 25       |
|    time_elapsed    | 314      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=210000, episode_reward=72.62 +/- 0.00
Episode length: 823.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 823        |
|    mean_reward          | 72.6       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 210000     |
| train/                  |            |
|    approx_kl            | 0.01711208 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.94      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0141     |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0153    |
|    std                  | 0.693      |
|    value_loss           | 0.00127    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 814      |
|    ep_rew_mean     | 44.8     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 650      |
|    iterations      | 26       |
|    time_elapsed    | 327      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=220000, episode_reward=75.60 +/- 0.00
Episode length: 795.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 795        |
|    mean_reward          | 75.6       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.01756939 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.87      |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0358    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0179    |
|    std                  | 0.675      |
|    value_loss           | 0.00103    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 792      |
|    ep_rew_mean     | 48       |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 651      |
|    iterations      | 27       |
|    time_elapsed    | 339      |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 768         |
|    ep_rew_mean          | 52          |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 656         |
|    iterations           | 28          |
|    time_elapsed         | 349         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.018684704 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0214     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0225     |
|    std                  | 0.657       |
|    value_loss           | 0.00112     |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=81.43 +/- 0.00
Episode length: 732.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 732         |
|    mean_reward          | 81.4        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.018106254 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0447     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0223     |
|    std                  | 0.643       |
|    value_loss           | 0.000954    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 749      |
|    ep_rew_mean     | 55.8     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 652      |
|    iterations      | 29       |
|    time_elapsed    | 364      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=240000, episode_reward=84.66 +/- 0.00
Episode length: 699.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 699        |
|    mean_reward          | 84.7       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.01809941 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.73      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0334    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.018     |
|    std                  | 0.627      |
|    value_loss           | 0.0122     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 729      |
|    ep_rew_mean     | 60       |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 649      |
|    iterations      | 30       |
|    time_elapsed    | 378      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=250000, episode_reward=91.60 +/- 0.00
Episode length: 704.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 704         |
|    mean_reward          | 91.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.019267825 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000671    |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0189     |
|    std                  | 0.608       |
|    value_loss           | 0.00124     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 722      |
|    ep_rew_mean     | 64.7     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 646      |
|    iterations      | 31       |
|    time_elapsed    | 392      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=260000, episode_reward=97.85 +/- 0.00
Episode length: 722.00 +/- 0.00
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 722       |
|    mean_reward          | 97.9      |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 260000    |
| train/                  |           |
|    approx_kl            | 0.0173227 |
|    clip_fraction        | 0.218     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.63     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0222   |
|    n_updates            | 310       |
|    policy_gradient_loss | -0.019    |
|    std                  | 0.593     |
|    value_loss           | 0.00092   |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 717      |
|    ep_rew_mean     | 69.7     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 644      |
|    iterations      | 32       |
|    time_elapsed    | 406      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=270000, episode_reward=103.90 +/- 0.00
Episode length: 773.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 773         |
|    mean_reward          | 104         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.017341238 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0442     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0193     |
|    std                  | 0.58        |
|    value_loss           | 0.000807    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 711      |
|    ep_rew_mean     | 74       |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 642      |
|    iterations      | 33       |
|    time_elapsed    | 420      |
|    total_timesteps | 270336   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 712        |
|    ep_rew_mean          | 78         |
|    success_rate         | 0          |
| time/                   |            |
|    fps                  | 643        |
|    iterations           | 34         |
|    time_elapsed         | 432        |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.01706507 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.54      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0156    |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0226    |
|    std                  | 0.568      |
|    value_loss           | 0.000817   |
----------------------------------------
Eval num_timesteps=280000, episode_reward=127.06 +/- 0.00
Episode length: 1637.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.64e+03    |
|    mean_reward          | 127         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.020424683 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.015      |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0198     |
|    std                  | 0.552       |
|    value_loss           | 0.000744    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 725      |
|    ep_rew_mean     | 80.8     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 641      |
|    iterations      | 35       |
|    time_elapsed    | 447      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=290000, episode_reward=130.40 +/- 0.00
Episode length: 1274.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.27e+03    |
|    mean_reward          | 130         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.016868819 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0168     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00651    |
|    std                  | 0.547       |
|    value_loss           | 0.00158     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 767      |
|    ep_rew_mean     | 84.3     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 639      |
|    iterations      | 36       |
|    time_elapsed    | 461      |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=300000, episode_reward=133.03 +/- 0.00
Episode length: 1130.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | 133         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.015160684 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0132     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00312    |
|    std                  | 0.545       |
|    value_loss           | 0.00184     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 790      |
|    ep_rew_mean     | 88.8     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 638      |
|    iterations      | 37       |
|    time_elapsed    | 474      |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=310000, episode_reward=135.23 +/- 0.00
Episode length: 1000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 135         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.015948024 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0182     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00756    |
|    std                  | 0.529       |
|    value_loss           | 0.000857    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 812      |
|    ep_rew_mean     | 93.1     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 637      |
|    iterations      | 38       |
|    time_elapsed    | 487      |
|    total_timesteps | 311296   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 831         |
|    ep_rew_mean          | 98          |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 641         |
|    iterations           | 39          |
|    time_elapsed         | 498         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.014210424 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0347     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00741    |
|    std                  | 0.52        |
|    value_loss           | 0.00101     |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=139.21 +/- 0.00
Episode length: 877.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 877        |
|    mean_reward          | 139        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.01404501 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00872   |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.00732   |
|    std                  | 0.519      |
|    value_loss           | 0.0184     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 849      |
|    ep_rew_mean     | 103      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 640      |
|    iterations      | 40       |
|    time_elapsed    | 511      |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=330000, episode_reward=143.33 +/- 0.00
Episode length: 814.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 814         |
|    mean_reward          | 143         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.013212481 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0121     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00408    |
|    std                  | 0.514       |
|    value_loss           | 0.00236     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 864      |
|    ep_rew_mean     | 109      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 638      |
|    iterations      | 41       |
|    time_elapsed    | 526      |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=340000, episode_reward=148.07 +/- 0.00
Episode length: 744.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 744         |
|    mean_reward          | 148         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.015875183 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0329     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0124     |
|    std                  | 0.507       |
|    value_loss           | 0.000887    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 870      |
|    ep_rew_mean     | 115      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 639      |
|    iterations      | 42       |
|    time_elapsed    | 537      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=350000, episode_reward=151.04 +/- 0.00
Episode length: 730.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 730         |
|    mean_reward          | 151         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.017180843 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0458     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0133     |
|    std                  | 0.493       |
|    value_loss           | 0.000814    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 871      |
|    ep_rew_mean     | 121      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 640      |
|    iterations      | 43       |
|    time_elapsed    | 549      |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=360000, episode_reward=155.45 +/- 0.00
Episode length: 674.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 674         |
|    mean_reward          | 155         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.018095639 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00465     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0122     |
|    std                  | 0.483       |
|    value_loss           | 0.000654    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 863      |
|    ep_rew_mean     | 127      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 641      |
|    iterations      | 44       |
|    time_elapsed    | 561      |
|    total_timesteps | 360448   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 840         |
|    ep_rew_mean          | 133         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 644         |
|    iterations           | 45          |
|    time_elapsed         | 571         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.015663648 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0215     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0127     |
|    std                  | 0.472       |
|    value_loss           | 0.000603    |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=160.34 +/- 0.00
Episode length: 621.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 621         |
|    mean_reward          | 160         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.014950421 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0213     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0112     |
|    std                  | 0.462       |
|    value_loss           | 0.000685    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 770      |
|    ep_rew_mean     | 138      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 645      |
|    iterations      | 46       |
|    time_elapsed    | 583      |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=380000, episode_reward=162.42 +/- 0.00
Episode length: 634.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 634        |
|    mean_reward          | 162        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.01563991 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0388    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.00986   |
|    std                  | 0.45       |
|    value_loss           | 0.000753   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 727      |
|    ep_rew_mean     | 143      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 646      |
|    iterations      | 47       |
|    time_elapsed    | 595      |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=390000, episode_reward=165.57 +/- 0.00
Episode length: 606.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 606         |
|    mean_reward          | 166         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.014314853 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00602    |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00758    |
|    std                  | 0.437       |
|    value_loss           | 0.000802    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 698      |
|    ep_rew_mean     | 148      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 647      |
|    iterations      | 48       |
|    time_elapsed    | 606      |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=400000, episode_reward=168.63 +/- 0.00
Episode length: 600.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 600         |
|    mean_reward          | 169         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.014953624 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00214    |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.007      |
|    std                  | 0.428       |
|    value_loss           | 0.000844    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 671      |
|    ep_rew_mean     | 151      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 648      |
|    iterations      | 49       |
|    time_elapsed    | 618      |
|    total_timesteps | 401408   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 646         |
|    ep_rew_mean          | 154         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 651         |
|    iterations           | 50          |
|    time_elapsed         | 628         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.012918267 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00305    |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00583    |
|    std                  | 0.425       |
|    value_loss           | 0.000822    |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=169.00 +/- 0.00
Episode length: 602.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 602         |
|    mean_reward          | 169         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.014516592 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.995      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00738    |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0084     |
|    std                  | 0.416       |
|    value_loss           | 0.000672    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 631      |
|    ep_rew_mean     | 157      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 652      |
|    iterations      | 51       |
|    time_elapsed    | 639      |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=420000, episode_reward=170.77 +/- 0.00
Episode length: 596.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 596         |
|    mean_reward          | 171         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.015541572 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.944      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.011      |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00627    |
|    std                  | 0.403       |
|    value_loss           | 0.000628    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 620      |
|    ep_rew_mean     | 159      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 653      |
|    iterations      | 52       |
|    time_elapsed    | 651      |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=430000, episode_reward=172.95 +/- 0.00
Episode length: 584.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 584         |
|    mean_reward          | 173         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.015166264 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.892      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0365     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00662    |
|    std                  | 0.391       |
|    value_loss           | 0.000803    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 612      |
|    ep_rew_mean     | 161      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 654      |
|    iterations      | 53       |
|    time_elapsed    | 662      |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=440000, episode_reward=173.90 +/- 0.00
Episode length: 565.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 565         |
|    mean_reward          | 174         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.014852875 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00109     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00599    |
|    std                  | 0.383       |
|    value_loss           | 0.000587    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 604      |
|    ep_rew_mean     | 163      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 655      |
|    iterations      | 54       |
|    time_elapsed    | 674      |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=450000, episode_reward=176.87 +/- 0.00
Episode length: 577.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 577         |
|    mean_reward          | 177         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.014722729 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.812      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00404    |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00647    |
|    std                  | 0.376       |
|    value_loss           | 0.00046     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 164      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 656      |
|    iterations      | 55       |
|    time_elapsed    | 685      |
|    total_timesteps | 450560   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 607         |
|    ep_rew_mean          | 166         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 659         |
|    iterations           | 56          |
|    time_elapsed         | 695         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.016707422 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.77       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0201     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00772    |
|    std                  | 0.367       |
|    value_loss           | 0.000456    |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=178.87 +/- 0.00
Episode length: 588.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 588         |
|    mean_reward          | 179         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.014055515 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.772      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0148      |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00179    |
|    std                  | 0.371       |
|    value_loss           | 0.00103     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 623      |
|    ep_rew_mean     | 169      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 660      |
|    iterations      | 57       |
|    time_elapsed    | 707      |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=470000, episode_reward=179.40 +/- 0.00
Episode length: 591.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 591         |
|    mean_reward          | 179         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.013164144 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0225     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00372    |
|    std                  | 0.365       |
|    value_loss           | 0.000756    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 636      |
|    ep_rew_mean     | 171      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 660      |
|    iterations      | 58       |
|    time_elapsed    | 718      |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=480000, episode_reward=225.43 +/- 0.00
Episode length: 984.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 984         |
|    mean_reward          | 225         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.015984805 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.736      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0297     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0061     |
|    std                  | 0.362       |
|    value_loss           | 0.000946    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 652      |
|    ep_rew_mean     | 175      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 660      |
|    iterations      | 59       |
|    time_elapsed    | 731      |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=490000, episode_reward=223.86 +/- 0.00
Episode length: 905.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 905         |
|    mean_reward          | 224         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.013735523 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0114     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00263    |
|    std                  | 0.357       |
|    value_loss           | 0.00117     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 674      |
|    ep_rew_mean     | 178      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 659      |
|    iterations      | 60       |
|    time_elapsed    | 744      |
|    total_timesteps | 491520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 700        |
|    ep_rew_mean          | 183        |
|    success_rate         | 0          |
| time/                   |            |
|    fps                  | 662        |
|    iterations           | 61         |
|    time_elapsed         | 754        |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.01683105 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.669     |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0122     |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0051    |
|    std                  | 0.349      |
|    value_loss           | 0.00168    |
----------------------------------------
Eval num_timesteps=500000, episode_reward=236.72 +/- 0.00
Episode length: 973.00 +/- 0.00
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 973       |
|    mean_reward          | 237       |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 500000    |
| train/                  |           |
|    approx_kl            | 0.0149155 |
|    clip_fraction        | 0.162     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.633    |
|    explained_variance   | 0.997     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0121   |
|    n_updates            | 610       |
|    policy_gradient_loss | -0.00285  |
|    std                  | 0.344     |
|    value_loss           | 0.00112   |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 729      |
|    ep_rew_mean     | 189      |
|    success_rate    | 0.02     |
| time/              |          |
|    fps             | 661      |
|    iterations      | 62       |
|    time_elapsed    | 767      |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=510000, episode_reward=272.44 +/- 0.00
Episode length: 1002.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 272         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.016731394 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.577      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0269      |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00342    |
|    std                  | 0.333       |
|    value_loss           | 0.00703     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 764      |
|    ep_rew_mean     | 196      |
|    success_rate    | 0.06     |
| time/              |          |
|    fps             | 660      |
|    iterations      | 63       |
|    time_elapsed    | 780      |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=520000, episode_reward=269.36 +/- 0.00
Episode length: 913.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 913         |
|    mean_reward          | 269         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 520000      |
| train/                  |             |
|    approx_kl            | 0.019231549 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.524      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0255      |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00468    |
|    std                  | 0.326       |
|    value_loss           | 0.0145      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 797      |
|    ep_rew_mean     | 203      |
|    success_rate    | 0.12     |
| time/              |          |
|    fps             | 659      |
|    iterations      | 64       |
|    time_elapsed    | 795      |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=530000, episode_reward=266.81 +/- 0.00
Episode length: 861.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 861         |
|    mean_reward          | 267         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 530000      |
| train/                  |             |
|    approx_kl            | 0.018397314 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.468      |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0115      |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00499    |
|    std                  | 0.318       |
|    value_loss           | 0.0151      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 835      |
|    ep_rew_mean     | 213      |
|    success_rate    | 0.18     |
| time/              |          |
|    fps             | 659      |
|    iterations      | 65       |
|    time_elapsed    | 807      |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=540000, episode_reward=268.50 +/- 0.00
Episode length: 837.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 837         |
|    mean_reward          | 268         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 540000      |
| train/                  |             |
|    approx_kl            | 0.014933595 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.42       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00451    |
|    std                  | 0.312       |
|    value_loss           | 0.00868     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 868      |
|    ep_rew_mean     | 222      |
|    success_rate    | 0.25     |
| time/              |          |
|    fps             | 659      |
|    iterations      | 66       |
|    time_elapsed    | 820      |
|    total_timesteps | 540672   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 879       |
|    ep_rew_mean          | 229       |
|    success_rate         | 0.29      |
| time/                   |           |
|    fps                  | 661       |
|    iterations           | 67        |
|    time_elapsed         | 830       |
|    total_timesteps      | 548864    |
| train/                  |           |
|    approx_kl            | 0.0155013 |
|    clip_fraction        | 0.172     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.379    |
|    explained_variance   | 0.986     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.036    |
|    n_updates            | 660       |
|    policy_gradient_loss | -0.00409  |
|    std                  | 0.306     |
|    value_loss           | 0.00465   |
---------------------------------------
Eval num_timesteps=550000, episode_reward=280.37 +/- 0.00
Episode length: 863.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 863         |
|    mean_reward          | 280         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.017327622 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.367      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0148      |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00375    |
|    std                  | 0.305       |
|    value_loss           | 0.00526     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 895      |
|    ep_rew_mean     | 237      |
|    success_rate    | 0.34     |
| time/              |          |
|    fps             | 660      |
|    iterations      | 68       |
|    time_elapsed    | 842      |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=560000, episode_reward=276.76 +/- 0.00
Episode length: 833.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 833         |
|    mean_reward          | 277         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.014256666 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.35       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0078     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00369    |
|    std                  | 0.303       |
|    value_loss           | 0.00448     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 913      |
|    ep_rew_mean     | 246      |
|    success_rate    | 0.4      |
| time/              |          |
|    fps             | 658      |
|    iterations      | 69       |
|    time_elapsed    | 857      |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=570000, episode_reward=276.30 +/- 0.00
Episode length: 826.00 +/- 0.00
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 826       |
|    mean_reward          | 276       |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 570000    |
| train/                  |           |
|    approx_kl            | 0.0142557 |
|    clip_fraction        | 0.174     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.321    |
|    explained_variance   | 0.99      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.000605 |
|    n_updates            | 690       |
|    policy_gradient_loss | -0.00308  |
|    std                  | 0.298     |
|    value_loss           | 0.00405   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 930      |
|    ep_rew_mean     | 255      |
|    success_rate    | 0.45     |
| time/              |          |
|    fps             | 657      |
|    iterations      | 70       |
|    time_elapsed    | 872      |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=580000, episode_reward=275.44 +/- 0.00
Episode length: 811.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | 275         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.017544942 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0176     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00045    |
|    std                  | 0.292       |
|    value_loss           | 0.00398     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 938      |
|    ep_rew_mean     | 262      |
|    success_rate    | 0.49     |
| time/              |          |
|    fps             | 655      |
|    iterations      | 71       |
|    time_elapsed    | 887      |
|    total_timesteps | 581632   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 936         |
|    ep_rew_mean          | 268         |
|    success_rate         | 0.51        |
| time/                   |             |
|    fps                  | 656         |
|    iterations           | 72          |
|    time_elapsed         | 899         |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.014810989 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.249      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0146      |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00101    |
|    std                  | 0.288       |
|    value_loss           | 0.00352     |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=275.52 +/- 0.00
Episode length: 786.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 786         |
|    mean_reward          | 276         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.014685165 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0262     |
|    n_updates            | 720         |
|    policy_gradient_loss | 0.000995    |
|    std                  | 0.285       |
|    value_loss           | 0.00376     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 926      |
|    ep_rew_mean     | 272      |
|    success_rate    | 0.52     |
| time/              |          |
|    fps             | 654      |
|    iterations      | 73       |
|    time_elapsed    | 913      |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=600000, episode_reward=274.48 +/- 0.00
Episode length: 783.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 783         |
|    mean_reward          | 274         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.014492736 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00535     |
|    n_updates            | 730         |
|    policy_gradient_loss | 0.000409    |
|    std                  | 0.281       |
|    value_loss           | 0.00253     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 909      |
|    ep_rew_mean     | 274      |
|    success_rate    | 0.49     |
| time/              |          |
|    fps             | 654      |
|    iterations      | 74       |
|    time_elapsed    | 926      |
|    total_timesteps | 606208   |
---------------------------------
Modele final sauvegarde: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/final_model.zip
Meilleur modele sauvegarde: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/best_model.zip
Statistiques VecNormalize: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/vecnormalize.pkl
