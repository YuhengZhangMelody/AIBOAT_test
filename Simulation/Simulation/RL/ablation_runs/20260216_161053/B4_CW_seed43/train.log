Using cpu device
Debut de l'entrainement...
Logging to /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/logs/tensorboard/PPO_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 328      |
|    ep_rew_mean     | -20.2    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 1772     |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=47.35 +/- 0.00
Episode length: 1289.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.29e+03    |
|    mean_reward          | 47.4        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.007327166 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.84       |
|    explained_variance   | -0.0727     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00158    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00658    |
|    std                  | 1           |
|    value_loss           | 0.065       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | -18.5    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 749      |
|    iterations      | 2        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=82.80 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 82.8        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011154537 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0227     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0165     |
|    std                  | 0.996       |
|    value_loss           | 0.0119      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 531      |
|    ep_rew_mean     | -18.4    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 599      |
|    iterations      | 3        |
|    time_elapsed    | 40       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=30000, episode_reward=10.07 +/- 0.00
Episode length: 279.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 279         |
|    mean_reward          | 10.1        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.018759992 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | 0.516       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00938     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0257     |
|    std                  | 0.98        |
|    value_loss           | 0.0101      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 562      |
|    ep_rew_mean     | -13.6    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 606      |
|    iterations      | 4        |
|    time_elapsed    | 54       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=40000, episode_reward=10.04 +/- 0.00
Episode length: 278.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 278         |
|    mean_reward          | 10          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.018820371 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0309     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.026      |
|    std                  | 0.955       |
|    value_loss           | 0.00849     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 454      |
|    ep_rew_mean     | -8.88    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 610      |
|    iterations      | 5        |
|    time_elapsed    | 67       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 451         |
|    ep_rew_mean          | -6.28       |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 621         |
|    iterations           | 6           |
|    time_elapsed         | 79          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.015258243 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0197     |
|    std                  | 0.945       |
|    value_loss           | 0.00889     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=11.57 +/- 0.00
Episode length: 318.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 318        |
|    mean_reward          | 11.6       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.01363257 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.7       |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0282    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0177    |
|    std                  | 0.942      |
|    value_loss           | 0.0107     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 1.19     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 619      |
|    iterations      | 7        |
|    time_elapsed    | 92       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=7.75 +/- 0.00
Episode length: 304.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 304         |
|    mean_reward          | 7.75        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.012842041 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.68       |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0118     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0168     |
|    std                  | 0.938       |
|    value_loss           | 0.0175      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 258      |
|    ep_rew_mean     | 2.86     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 633      |
|    iterations      | 8        |
|    time_elapsed    | 103      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=70000, episode_reward=7.76 +/- 0.00
Episode length: 322.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 322         |
|    mean_reward          | 7.76        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.011105879 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.67       |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00823    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0152     |
|    std                  | 0.936       |
|    value_loss           | 0.0191      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 4.76     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 644      |
|    iterations      | 9        |
|    time_elapsed    | 114      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=80000, episode_reward=7.97 +/- 0.00
Episode length: 330.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 330         |
|    mean_reward          | 7.97        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.013509128 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0224     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0149     |
|    std                  | 0.932       |
|    value_loss           | 0.0143      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 320      |
|    ep_rew_mean     | 6.9      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 653      |
|    iterations      | 10       |
|    time_elapsed    | 125      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=90000, episode_reward=9.49 +/- 0.00
Episode length: 341.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 341        |
|    mean_reward          | 9.49       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 90000      |
| train/                  |            |
|    approx_kl            | 0.01340498 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.64      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0165    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0193    |
|    std                  | 0.931      |
|    value_loss           | 0.00985    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 365      |
|    ep_rew_mean     | 9.54     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 661      |
|    iterations      | 11       |
|    time_elapsed    | 136      |
|    total_timesteps | 90112    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 434          |
|    ep_rew_mean          | 13.3         |
|    success_rate         | 0            |
| time/                   |              |
|    fps                  | 672          |
|    iterations           | 12           |
|    time_elapsed         | 146          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0144790765 |
|    clip_fraction        | 0.205        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.61        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0222      |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.0204      |
|    std                  | 0.927        |
|    value_loss           | 0.0126       |
------------------------------------------
Eval num_timesteps=100000, episode_reward=74.12 +/- 0.00
Episode length: 1118.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.12e+03    |
|    mean_reward          | 74.1        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.016454902 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0557     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0251     |
|    std                  | 0.917       |
|    value_loss           | 0.00707     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | 17.7     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 667      |
|    iterations      | 13       |
|    time_elapsed    | 159      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=110000, episode_reward=80.67 +/- 0.00
Episode length: 1017.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.02e+03    |
|    mean_reward          | 80.7        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.014949833 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0149     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0235     |
|    std                  | 0.91        |
|    value_loss           | 0.00792     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 554      |
|    ep_rew_mean     | 22.9     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 665      |
|    iterations      | 14       |
|    time_elapsed    | 172      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=120000, episode_reward=93.68 +/- 0.00
Episode length: 953.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 953         |
|    mean_reward          | 93.7        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.017409652 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0265     |
|    std                  | 0.891       |
|    value_loss           | 0.00752     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 615      |
|    ep_rew_mean     | 28.4     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 663      |
|    iterations      | 15       |
|    time_elapsed    | 185      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=130000, episode_reward=90.48 +/- 0.00
Episode length: 774.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 774         |
|    mean_reward          | 90.5        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.018460697 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000238   |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0284     |
|    std                  | 0.877       |
|    value_loss           | 0.00885     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 679      |
|    ep_rew_mean     | 35.8     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 664      |
|    iterations      | 16       |
|    time_elapsed    | 197      |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 741         |
|    ep_rew_mean          | 43.8        |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 673         |
|    iterations           | 17          |
|    time_elapsed         | 206         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.018977553 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.44       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0433     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0246     |
|    std                  | 0.856       |
|    value_loss           | 0.0143      |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=112.69 +/- 0.00
Episode length: 1163.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.16e+03   |
|    mean_reward          | 113        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.01908119 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.39      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0312    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0247    |
|    std                  | 0.835      |
|    value_loss           | 0.0093     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 783      |
|    ep_rew_mean     | 49.7     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 669      |
|    iterations      | 18       |
|    time_elapsed    | 220      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=101.83 +/- 0.00
Episode length: 625.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 625         |
|    mean_reward          | 102         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.017051894 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0112      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0135     |
|    std                  | 0.813       |
|    value_loss           | 0.00809     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 824      |
|    ep_rew_mean     | 57.7     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 670      |
|    iterations      | 19       |
|    time_elapsed    | 232      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=160000, episode_reward=113.29 +/- 0.00
Episode length: 1058.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.06e+03    |
|    mean_reward          | 113         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.017942894 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00725     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0134     |
|    std                  | 0.793       |
|    value_loss           | 0.00947     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 836      |
|    ep_rew_mean     | 63.1     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 668      |
|    iterations      | 20       |
|    time_elapsed    | 245      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=170000, episode_reward=113.45 +/- 0.00
Episode length: 1060.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.06e+03    |
|    mean_reward          | 113         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.013690723 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00833     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0116     |
|    std                  | 0.772       |
|    value_loss           | 0.00622     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 860      |
|    ep_rew_mean     | 68.5     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 666      |
|    iterations      | 21       |
|    time_elapsed    | 258      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=180000, episode_reward=110.70 +/- 0.00
Episode length: 1079.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.08e+03    |
|    mean_reward          | 111         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.013776332 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.014       |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00993    |
|    std                  | 0.755       |
|    value_loss           | 0.0039      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 852      |
|    ep_rew_mean     | 72.3     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 664      |
|    iterations      | 22       |
|    time_elapsed    | 271      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 838         |
|    ep_rew_mean          | 77.8        |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 670         |
|    iterations           | 23          |
|    time_elapsed         | 281         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.013567936 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0218     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0108     |
|    std                  | 0.741       |
|    value_loss           | 0.00357     |
-----------------------------------------
Eval num_timesteps=190000, episode_reward=113.44 +/- 0.00
Episode length: 1027.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.03e+03    |
|    mean_reward          | 113         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 0.015240524 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0132     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0119     |
|    std                  | 0.716       |
|    value_loss           | 0.00567     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 825      |
|    ep_rew_mean     | 83.3     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 668      |
|    iterations      | 24       |
|    time_elapsed    | 293      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=121.31 +/- 0.00
Episode length: 831.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 831         |
|    mean_reward          | 121         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.013045054 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0121     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0122     |
|    std                  | 0.698       |
|    value_loss           | 0.00497     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 815      |
|    ep_rew_mean     | 86.9     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 668      |
|    iterations      | 25       |
|    time_elapsed    | 306      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=210000, episode_reward=122.10 +/- 0.00
Episode length: 828.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 828        |
|    mean_reward          | 122        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 210000     |
| train/                  |            |
|    approx_kl            | 0.01367977 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.04      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00932    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0122    |
|    std                  | 0.681      |
|    value_loss           | 0.00233    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 819      |
|    ep_rew_mean     | 91.6     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 668      |
|    iterations      | 26       |
|    time_elapsed    | 318      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=220000, episode_reward=118.73 +/- 0.00
Episode length: 928.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 928         |
|    mean_reward          | 119         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.014147233 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0106      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0118     |
|    std                  | 0.661       |
|    value_loss           | 0.00218     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 811      |
|    ep_rew_mean     | 94.8     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 668      |
|    iterations      | 27       |
|    time_elapsed    | 331      |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 797         |
|    ep_rew_mean          | 98.4        |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 668         |
|    iterations           | 28          |
|    time_elapsed         | 343         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.012879543 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00253     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0107     |
|    std                  | 0.648       |
|    value_loss           | 0.0135      |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=127.83 +/- 0.00
Episode length: 735.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 735          |
|    mean_reward          | 128          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 230000       |
| train/                  |              |
|    approx_kl            | 0.0126319155 |
|    clip_fraction        | 0.15         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.9         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0133      |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00755     |
|    std                  | 0.635        |
|    value_loss           | 0.0014       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 784      |
|    ep_rew_mean     | 102      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 665      |
|    iterations      | 29       |
|    time_elapsed    | 357      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=240000, episode_reward=128.26 +/- 0.00
Episode length: 715.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 715         |
|    mean_reward          | 128         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.014324505 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00985     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.01       |
|    std                  | 0.609       |
|    value_loss           | 0.00122     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 759      |
|    ep_rew_mean     | 105      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 665      |
|    iterations      | 30       |
|    time_elapsed    | 369      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=250000, episode_reward=129.31 +/- 0.00
Episode length: 702.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 702         |
|    mean_reward          | 129         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.015731443 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0145     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0108     |
|    std                  | 0.589       |
|    value_loss           | 0.000779    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 730      |
|    ep_rew_mean     | 109      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 665      |
|    iterations      | 31       |
|    time_elapsed    | 381      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=260000, episode_reward=130.69 +/- 0.00
Episode length: 682.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 682         |
|    mean_reward          | 131         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 260000      |
| train/                  |             |
|    approx_kl            | 0.015898008 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00061     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0145     |
|    std                  | 0.569       |
|    value_loss           | 0.000765    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 716      |
|    ep_rew_mean     | 111      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 666      |
|    iterations      | 32       |
|    time_elapsed    | 393      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=270000, episode_reward=131.62 +/- 0.00
Episode length: 652.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 652         |
|    mean_reward          | 132         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.015118316 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0145     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0128     |
|    std                  | 0.555       |
|    value_loss           | 0.00066     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 700      |
|    ep_rew_mean     | 114      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 667      |
|    iterations      | 33       |
|    time_elapsed    | 405      |
|    total_timesteps | 270336   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 685         |
|    ep_rew_mean          | 116         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 671         |
|    iterations           | 34          |
|    time_elapsed         | 414         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.016926672 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00733    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0157     |
|    std                  | 0.545       |
|    value_loss           | 0.000642    |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=133.37 +/- 0.00
Episode length: 617.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 617        |
|    mean_reward          | 133        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.01660488 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00517   |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0134    |
|    std                  | 0.526      |
|    value_loss           | 0.000633   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 664      |
|    ep_rew_mean     | 118      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 672      |
|    iterations      | 35       |
|    time_elapsed    | 426      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=290000, episode_reward=133.90 +/- 0.00
Episode length: 607.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 607         |
|    mean_reward          | 134         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.014224211 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0303     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00983    |
|    std                  | 0.514       |
|    value_loss           | 0.0139      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 647      |
|    ep_rew_mean     | 120      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 673      |
|    iterations      | 36       |
|    time_elapsed    | 437      |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=300000, episode_reward=134.23 +/- 0.00
Episode length: 600.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 134          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 300000       |
| train/                  |              |
|    approx_kl            | 0.0155445775 |
|    clip_fraction        | 0.187        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.44        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00217     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00859     |
|    std                  | 0.495        |
|    value_loss           | 0.000793     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 632      |
|    ep_rew_mean     | 122      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 674      |
|    iterations      | 37       |
|    time_elapsed    | 449      |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=310000, episode_reward=134.61 +/- 0.00
Episode length: 603.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 603         |
|    mean_reward          | 135         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.015549288 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0131     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00937    |
|    std                  | 0.488       |
|    value_loss           | 0.000606    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 618      |
|    ep_rew_mean     | 123      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 675      |
|    iterations      | 38       |
|    time_elapsed    | 461      |
|    total_timesteps | 311296   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 607         |
|    ep_rew_mean          | 124         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 678         |
|    iterations           | 39          |
|    time_elapsed         | 470         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.015878666 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0264     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0131     |
|    std                  | 0.472       |
|    value_loss           | 0.000578    |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=137.35 +/- 0.00
Episode length: 575.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 575        |
|    mean_reward          | 137        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.01882667 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0168    |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0134    |
|    std                  | 0.46       |
|    value_loss           | 0.000495   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 596      |
|    ep_rew_mean     | 126      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 679      |
|    iterations      | 40       |
|    time_elapsed    | 482      |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=330000, episode_reward=138.50 +/- 0.00
Episode length: 553.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 553        |
|    mean_reward          | 138        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.01833308 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.018     |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.015     |
|    std                  | 0.448      |
|    value_loss           | 0.000572   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 585      |
|    ep_rew_mean     | 127      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 679      |
|    iterations      | 41       |
|    time_elapsed    | 494      |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=340000, episode_reward=139.67 +/- 0.00
Episode length: 544.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 544         |
|    mean_reward          | 140         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.019456383 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00921     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0141     |
|    std                  | 0.435       |
|    value_loss           | 0.000505    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 580      |
|    ep_rew_mean     | 129      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 680      |
|    iterations      | 42       |
|    time_elapsed    | 505      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=350000, episode_reward=139.61 +/- 0.00
Episode length: 534.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 534         |
|    mean_reward          | 140         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.018076994 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0339     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0147     |
|    std                  | 0.424       |
|    value_loss           | 0.000437    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 571      |
|    ep_rew_mean     | 131      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 681      |
|    iterations      | 43       |
|    time_elapsed    | 517      |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=360000, episode_reward=139.93 +/- 0.00
Episode length: 532.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 532         |
|    mean_reward          | 140         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.019906567 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00724    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0141     |
|    std                  | 0.411       |
|    value_loss           | 0.000403    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 561      |
|    ep_rew_mean     | 132      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 682      |
|    iterations      | 44       |
|    time_elapsed    | 528      |
|    total_timesteps | 360448   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 548        |
|    ep_rew_mean          | 133        |
|    success_rate         | 0          |
| time/                   |            |
|    fps                  | 684        |
|    iterations           | 45         |
|    time_elapsed         | 538        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.01788356 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0034     |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.013     |
|    std                  | 0.4        |
|    value_loss           | 0.000392   |
----------------------------------------
Eval num_timesteps=370000, episode_reward=142.34 +/- 0.00
Episode length: 503.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 503         |
|    mean_reward          | 142         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.016579371 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.946      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0161     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0085     |
|    std                  | 0.386       |
|    value_loss           | 0.000325    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 535      |
|    ep_rew_mean     | 134      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 685      |
|    iterations      | 46       |
|    time_elapsed    | 549      |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=380000, episode_reward=143.90 +/- 0.00
Episode length: 484.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 484        |
|    mean_reward          | 144        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.01979293 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.862     |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0134     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0115    |
|    std                  | 0.369      |
|    value_loss           | 0.000399   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 522      |
|    ep_rew_mean     | 136      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 686      |
|    iterations      | 47       |
|    time_elapsed    | 560      |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=390000, episode_reward=145.34 +/- 0.00
Episode length: 480.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 480         |
|    mean_reward          | 145         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.017836045 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.79       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0232     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00752    |
|    std                  | 0.356       |
|    value_loss           | 0.000383    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | 137      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 687      |
|    iterations      | 48       |
|    time_elapsed    | 572      |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=400000, episode_reward=147.05 +/- 0.00
Episode length: 477.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 477         |
|    mean_reward          | 147         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.017397787 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.74       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0158      |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00714    |
|    std                  | 0.349       |
|    value_loss           | 0.000328    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | 138      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 688      |
|    iterations      | 49       |
|    time_elapsed    | 583      |
|    total_timesteps | 401408   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 497         |
|    ep_rew_mean          | 140         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 690         |
|    iterations           | 50          |
|    time_elapsed         | 593         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.014666993 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.686      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00444    |
|    std                  | 0.34        |
|    value_loss           | 0.000176    |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=148.91 +/- 0.00
Episode length: 479.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 479         |
|    mean_reward          | 149         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.015665477 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.633      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000602    |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00543    |
|    std                  | 0.33        |
|    value_loss           | 0.000232    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | 141      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 691      |
|    iterations      | 51       |
|    time_elapsed    | 604      |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=420000, episode_reward=149.43 +/- 0.00
Episode length: 475.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 475         |
|    mean_reward          | 149         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.014621773 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.577      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00607     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00373    |
|    std                  | 0.322       |
|    value_loss           | 0.000216    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | 142      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 692      |
|    iterations      | 52       |
|    time_elapsed    | 615      |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=430000, episode_reward=150.63 +/- 0.00
Episode length: 475.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 475         |
|    mean_reward          | 151         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.012565868 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.524      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0225     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00404    |
|    std                  | 0.314       |
|    value_loss           | 0.000277    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 143      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 692      |
|    iterations      | 53       |
|    time_elapsed    | 626      |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=440000, episode_reward=150.47 +/- 0.00
Episode length: 484.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 484         |
|    mean_reward          | 150         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.016964234 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.475      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0156      |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00592    |
|    std                  | 0.305       |
|    value_loss           | 0.000307    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | 144      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 693      |
|    iterations      | 54       |
|    time_elapsed    | 637      |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=450000, episode_reward=152.14 +/- 0.00
Episode length: 482.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 482         |
|    mean_reward          | 152         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.016193397 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.424      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0644     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00451    |
|    std                  | 0.298       |
|    value_loss           | 0.000241    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | 145      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 694      |
|    iterations      | 55       |
|    time_elapsed    | 648      |
|    total_timesteps | 450560   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 483         |
|    ep_rew_mean          | 147         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 696         |
|    iterations           | 56          |
|    time_elapsed         | 658         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.012438184 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.397      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00953    |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00323    |
|    std                  | 0.295       |
|    value_loss           | 0.000221    |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=155.50 +/- 0.00
Episode length: 473.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 473         |
|    mean_reward          | 155         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.015598839 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.345      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0025      |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00522    |
|    std                  | 0.286       |
|    value_loss           | 0.000244    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 148      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 696      |
|    iterations      | 57       |
|    time_elapsed    | 669      |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=470000, episode_reward=157.04 +/- 0.00
Episode length: 474.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | 157         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.012644952 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.287      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00182    |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00214    |
|    std                  | 0.277       |
|    value_loss           | 0.000302    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | 149      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 697      |
|    iterations      | 58       |
|    time_elapsed    | 681      |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=480000, episode_reward=157.31 +/- 0.00
Episode length: 480.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 480         |
|    mean_reward          | 157         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.015007382 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0101      |
|    n_updates            | 580         |
|    policy_gradient_loss | 2.9e-05     |
|    std                  | 0.275       |
|    value_loss           | 0.000936    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | 150      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 698      |
|    iterations      | 59       |
|    time_elapsed    | 692      |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=490000, episode_reward=157.61 +/- 0.00
Episode length: 478.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 478         |
|    mean_reward          | 158         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.016355617 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00552    |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00192    |
|    std                  | 0.266       |
|    value_loss           | 0.000287    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | 151      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 698      |
|    iterations      | 60       |
|    time_elapsed    | 703      |
|    total_timesteps | 491520   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 478          |
|    ep_rew_mean          | 150          |
|    success_rate         | 0            |
| time/                   |              |
|    fps                  | 700          |
|    iterations           | 61           |
|    time_elapsed         | 713          |
|    total_timesteps      | 499712       |
| train/                  |              |
|    approx_kl            | 0.0153962225 |
|    clip_fraction        | 0.185        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.144       |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0165       |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00293     |
|    std                  | 0.259        |
|    value_loss           | 0.000245     |
------------------------------------------
Eval num_timesteps=500000, episode_reward=157.78 +/- 0.00
Episode length: 484.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 484         |
|    mean_reward          | 158         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.021371152 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0977     |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00132    |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00122    |
|    std                  | 0.252       |
|    value_loss           | 0.0112      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | 149      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 700      |
|    iterations      | 62       |
|    time_elapsed    | 724      |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=510000, episode_reward=157.34 +/- 0.00
Episode length: 476.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 157         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.018348545 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0672     |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0277     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.000871   |
|    std                  | 0.25        |
|    value_loss           | 0.0135      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | 150      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 701      |
|    iterations      | 63       |
|    time_elapsed    | 736      |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=520000, episode_reward=158.59 +/- 0.00
Episode length: 472.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 472        |
|    mean_reward          | 159        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.01762242 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0225    |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0232    |
|    n_updates            | 630        |
|    policy_gradient_loss | 0.000369   |
|    std                  | 0.242      |
|    value_loss           | 0.000548   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | 151      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 701      |
|    iterations      | 64       |
|    time_elapsed    | 747      |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=530000, episode_reward=158.77 +/- 0.00
Episode length: 471.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 471         |
|    mean_reward          | 159         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 530000      |
| train/                  |             |
|    approx_kl            | 0.018490944 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.029       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0247     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00252    |
|    std                  | 0.238       |
|    value_loss           | 0.000211    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | 151      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 701      |
|    iterations      | 65       |
|    time_elapsed    | 758      |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=540000, episode_reward=159.35 +/- 0.00
Episode length: 477.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 477         |
|    mean_reward          | 159         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 540000      |
| train/                  |             |
|    approx_kl            | 0.017102627 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.0671      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0129      |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.000651   |
|    std                  | 0.233       |
|    value_loss           | 0.000153    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 150      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 702      |
|    iterations      | 66       |
|    time_elapsed    | 769      |
|    total_timesteps | 540672   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 485         |
|    ep_rew_mean          | 152         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 703         |
|    iterations           | 67          |
|    time_elapsed         | 779         |
|    total_timesteps      | 548864      |
| train/                  |             |
|    approx_kl            | 0.018807482 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.102       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0104      |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00189    |
|    std                  | 0.229       |
|    value_loss           | 0.0107      |
-----------------------------------------
Eval num_timesteps=550000, episode_reward=160.31 +/- 0.00
Episode length: 483.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 483        |
|    mean_reward          | 160        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 550000     |
| train/                  |            |
|    approx_kl            | 0.48862803 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.174      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0084    |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.0163    |
|    std                  | 0.218      |
|    value_loss           | 0.000818   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | 154      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 704      |
|    iterations      | 68       |
|    time_elapsed    | 790      |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=560000, episode_reward=160.17 +/- 0.00
Episode length: 488.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 488         |
|    mean_reward          | 160         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.018026497 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.245       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0181      |
|    n_updates            | 680         |
|    policy_gradient_loss | 0.0036      |
|    std                  | 0.212       |
|    value_loss           | 0.000188    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | 155      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 704      |
|    iterations      | 69       |
|    time_elapsed    | 802      |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=570000, episode_reward=270.83 +/- 0.00
Episode length: 807.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 807        |
|    mean_reward          | 271        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 570000     |
| train/                  |            |
|    approx_kl            | 0.01553878 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.292      |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0249     |
|    n_updates            | 690        |
|    policy_gradient_loss | 0.00177    |
|    std                  | 0.208      |
|    value_loss           | 0.000355   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | 156      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 704      |
|    iterations      | 70       |
|    time_elapsed    | 814      |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=580000, episode_reward=266.02 +/- 0.00
Episode length: 841.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 841         |
|    mean_reward          | 266         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.016093804 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.334       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0313     |
|    n_updates            | 700         |
|    policy_gradient_loss | 0.00876     |
|    std                  | 0.203       |
|    value_loss           | 0.00139     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 159      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 703      |
|    iterations      | 71       |
|    time_elapsed    | 826      |
|    total_timesteps | 581632   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 531         |
|    ep_rew_mean          | 164         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 705         |
|    iterations           | 72          |
|    time_elapsed         | 836         |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.015377197 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.399       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00341     |
|    n_updates            | 710         |
|    policy_gradient_loss | 0.00325     |
|    std                  | 0.196       |
|    value_loss           | 0.00258     |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=267.24 +/- 0.00
Episode length: 815.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 815         |
|    mean_reward          | 267         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.016284835 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.471       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00735     |
|    n_updates            | 720         |
|    policy_gradient_loss | 0.000339    |
|    std                  | 0.19        |
|    value_loss           | 0.00431     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 537      |
|    ep_rew_mean     | 166      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 704      |
|    iterations      | 73       |
|    time_elapsed    | 848      |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=600000, episode_reward=268.79 +/- 0.00
Episode length: 815.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 815         |
|    mean_reward          | 269         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.016794521 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.508       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0424      |
|    n_updates            | 730         |
|    policy_gradient_loss | 0.00556     |
|    std                  | 0.188       |
|    value_loss           | 0.00401     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 532      |
|    ep_rew_mean     | 167      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 704      |
|    iterations      | 74       |
|    time_elapsed    | 860      |
|    total_timesteps | 606208   |
---------------------------------
Modele final sauvegarde: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/final_model.zip
Meilleur modele sauvegarde: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/best_model.zip
Statistiques VecNormalize: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/vecnormalize.pkl
