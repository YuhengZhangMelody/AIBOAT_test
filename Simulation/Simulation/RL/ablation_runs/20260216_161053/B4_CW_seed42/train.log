Using cpu device
Debut de l'entrainement...
Logging to /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/logs/tensorboard/PPO_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 282      |
|    ep_rew_mean     | -16.1    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 1826     |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.20 +/- 0.00
Episode length: 237.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 237          |
|    mean_reward          | -0.204       |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0055248835 |
|    clip_fraction        | 0.0485       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.0239       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0148       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00344     |
|    std                  | 0.995        |
|    value_loss           | 0.108        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 405      |
|    ep_rew_mean     | -17.5    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 1036     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-21.70 +/- 0.00
Episode length: 797.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 797        |
|    mean_reward          | -21.7      |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.00891759 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.83      |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0158    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00904   |
|    std                  | 0.993      |
|    value_loss           | 0.00934    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 372      |
|    ep_rew_mean     | -17.9    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 845      |
|    iterations      | 3        |
|    time_elapsed    | 29       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=30000, episode_reward=4.51 +/- 0.00
Episode length: 233.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 233         |
|    mean_reward          | 4.51        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.008707972 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0052      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0107     |
|    std                  | 0.984       |
|    value_loss           | 0.0147      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | -13      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 821      |
|    iterations      | 4        |
|    time_elapsed    | 39       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=40000, episode_reward=9.18 +/- 0.00
Episode length: 268.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | 9.18        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.017285693 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.79       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00403    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0245     |
|    std                  | 0.968       |
|    value_loss           | 0.0116      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | -5.41    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 811      |
|    iterations      | 5        |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | -1.02       |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 816         |
|    iterations           | 6           |
|    time_elapsed         | 60          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.016823754 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.74       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0426     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0248     |
|    std                  | 0.949       |
|    value_loss           | 0.00569     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=12.23 +/- 0.00
Episode length: 342.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 342        |
|    mean_reward          | 12.2       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.01557664 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.72      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0289    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0199    |
|    std                  | 0.944      |
|    value_loss           | 0.00544    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 243      |
|    ep_rew_mean     | -0.455   |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 806      |
|    iterations      | 7        |
|    time_elapsed    | 71       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=66.05 +/- 0.00
Episode length: 1875.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.88e+03    |
|    mean_reward          | 66          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.010600293 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000796    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0123     |
|    std                  | 0.95        |
|    value_loss           | 0.00885     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.0378   |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 757      |
|    iterations      | 8        |
|    time_elapsed    | 86       |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=70000, episode_reward=67.77 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 67.8        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.011915937 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0127     |
|    std                  | 0.949       |
|    value_loss           | 0.00907     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 362      |
|    ep_rew_mean     | 0.67     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 721      |
|    iterations      | 9        |
|    time_elapsed    | 102      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=80000, episode_reward=57.19 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 57.2        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.017045934 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.68       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.014      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0216     |
|    std                  | 0.939       |
|    value_loss           | 0.00519     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 427      |
|    ep_rew_mean     | 2.34     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 696      |
|    iterations      | 10       |
|    time_elapsed    | 117      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=90000, episode_reward=60.13 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 60.1        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.016128555 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00704    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0204     |
|    std                  | 0.937       |
|    value_loss           | 0.00301     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | 4.23     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 676      |
|    iterations      | 11       |
|    time_elapsed    | 133      |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 560         |
|    ep_rew_mean          | 6.65        |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 688         |
|    iterations           | 12          |
|    time_elapsed         | 142         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.016632162 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00517    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0253     |
|    std                  | 0.946       |
|    value_loss           | 0.00364     |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=11.38 +/- 0.00
Episode length: 259.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 259        |
|    mean_reward          | 11.4       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.01803815 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.64      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0103    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0263    |
|    std                  | 0.942      |
|    value_loss           | 0.00413    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 636      |
|    ep_rew_mean     | 9.4      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 694      |
|    iterations      | 13       |
|    time_elapsed    | 153      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=110000, episode_reward=10.86 +/- 0.00
Episode length: 186.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 186          |
|    mean_reward          | 10.9         |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 110000       |
| train/                  |              |
|    approx_kl            | 0.0149054825 |
|    clip_fraction        | 0.197        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.62        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0134      |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.0144      |
|    std                  | 0.931        |
|    value_loss           | 0.0079       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 721      |
|    ep_rew_mean     | 13.7     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 700      |
|    iterations      | 14       |
|    time_elapsed    | 163      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=120000, episode_reward=12.55 +/- 0.00
Episode length: 181.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 181         |
|    mean_reward          | 12.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.014610641 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.61       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0157     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0138     |
|    std                  | 0.93        |
|    value_loss           | 0.0163      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 767      |
|    ep_rew_mean     | 17.6     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 706      |
|    iterations      | 15       |
|    time_elapsed    | 174      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=130000, episode_reward=132.85 +/- 0.00
Episode length: 1492.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.49e+03    |
|    mean_reward          | 133         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.016547306 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.61       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00401    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0178     |
|    std                  | 0.938       |
|    value_loss           | 0.0232      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 835      |
|    ep_rew_mean     | 23.1     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 693      |
|    iterations      | 16       |
|    time_elapsed    | 189      |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 905         |
|    ep_rew_mean          | 30.4        |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 692         |
|    iterations           | 17          |
|    time_elapsed         | 201         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.019113284 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0103     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0205     |
|    std                  | 0.911       |
|    value_loss           | 0.0101      |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=141.75 +/- 0.00
Episode length: 1150.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.15e+03   |
|    mean_reward          | 142        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.01606441 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.53      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00555   |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0169    |
|    std                  | 0.891      |
|    value_loss           | 0.0162     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 38       |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 679      |
|    iterations      | 18       |
|    time_elapsed    | 216      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=146.79 +/- 0.00
Episode length: 1083.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.08e+03    |
|    mean_reward          | 147         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.016302388 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00777    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0201     |
|    std                  | 0.871       |
|    value_loss           | 0.00723     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.04e+03 |
|    ep_rew_mean     | 47.3     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 674      |
|    iterations      | 19       |
|    time_elapsed    | 230      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=160000, episode_reward=151.69 +/- 0.00
Episode length: 1021.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.02e+03   |
|    mean_reward          | 152        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.01747328 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.44      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0182    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0198    |
|    std                  | 0.85       |
|    value_loss           | 0.00794    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.09e+03 |
|    ep_rew_mean     | 57.1     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 665      |
|    iterations      | 20       |
|    time_elapsed    | 246      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=170000, episode_reward=156.37 +/- 0.00
Episode length: 976.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 976         |
|    mean_reward          | 156         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.017824512 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0094     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0167     |
|    std                  | 0.83        |
|    value_loss           | 0.0156      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.11e+03 |
|    ep_rew_mean     | 67.8     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 658      |
|    iterations      | 21       |
|    time_elapsed    | 261      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=180000, episode_reward=159.45 +/- 0.00
Episode length: 928.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 928         |
|    mean_reward          | 159         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.016509883 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0094      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0155     |
|    std                  | 0.814       |
|    value_loss           | 0.0104      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.1e+03  |
|    ep_rew_mean     | 80.9     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 657      |
|    iterations      | 22       |
|    time_elapsed    | 274      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.04e+03    |
|    ep_rew_mean          | 92          |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 664         |
|    iterations           | 23          |
|    time_elapsed         | 283         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.019578144 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0129      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0179     |
|    std                  | 0.785       |
|    value_loss           | 0.00635     |
-----------------------------------------
Eval num_timesteps=190000, episode_reward=165.56 +/- 0.00
Episode length: 865.00 +/- 0.00
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 865       |
|    mean_reward          | 166       |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 190000    |
| train/                  |           |
|    approx_kl            | 0.0159309 |
|    clip_fraction        | 0.204     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.27     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00815  |
|    n_updates            | 230       |
|    policy_gradient_loss | -0.0152   |
|    std                  | 0.765     |
|    value_loss           | 0.00916   |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | 100      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 662      |
|    iterations      | 24       |
|    time_elapsed    | 296      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=168.35 +/- 0.00
Episode length: 847.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 847         |
|    mean_reward          | 168         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.014422384 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0255     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0136     |
|    std                  | 0.742       |
|    value_loss           | 0.00542     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 932      |
|    ep_rew_mean     | 110      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 658      |
|    iterations      | 25       |
|    time_elapsed    | 310      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=210000, episode_reward=171.32 +/- 0.00
Episode length: 822.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 822         |
|    mean_reward          | 171         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.017813582 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0164     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0177     |
|    std                  | 0.716       |
|    value_loss           | 0.00201     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 945      |
|    ep_rew_mean     | 122      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 654      |
|    iterations      | 26       |
|    time_elapsed    | 325      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=220000, episode_reward=175.66 +/- 0.00
Episode length: 804.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 804         |
|    mean_reward          | 176         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.016448604 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0215      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0173     |
|    std                  | 0.701       |
|    value_loss           | 0.00176     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 896      |
|    ep_rew_mean     | 130      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 651      |
|    iterations      | 27       |
|    time_elapsed    | 339      |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 890         |
|    ep_rew_mean          | 139         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 652         |
|    iterations           | 28          |
|    time_elapsed         | 351         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.016401581 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00209     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0181     |
|    std                  | 0.678       |
|    value_loss           | 0.00153     |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=185.80 +/- 0.00
Episode length: 768.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 768         |
|    mean_reward          | 186         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.015124746 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00966    |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0154     |
|    std                  | 0.661       |
|    value_loss           | 0.00146     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 857      |
|    ep_rew_mean     | 145      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 649      |
|    iterations      | 29       |
|    time_elapsed    | 366      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=240000, episode_reward=194.01 +/- 0.00
Episode length: 788.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 788         |
|    mean_reward          | 194         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.013863643 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0102     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0128     |
|    std                  | 0.648       |
|    value_loss           | 0.00156     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 842      |
|    ep_rew_mean     | 152      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 649      |
|    iterations      | 30       |
|    time_elapsed    | 378      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=250000, episode_reward=197.84 +/- 0.00
Episode length: 765.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 765         |
|    mean_reward          | 198         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.014510573 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0323     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0142     |
|    std                  | 0.631       |
|    value_loss           | 0.00128     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 816      |
|    ep_rew_mean     | 156      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 646      |
|    iterations      | 31       |
|    time_elapsed    | 392      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=260000, episode_reward=201.74 +/- 0.00
Episode length: 773.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 773         |
|    mean_reward          | 202         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 260000      |
| train/                  |             |
|    approx_kl            | 0.014424192 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.011      |
|    std                  | 0.615       |
|    value_loss           | 0.0101      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 801      |
|    ep_rew_mean     | 162      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 643      |
|    iterations      | 32       |
|    time_elapsed    | 407      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=270000, episode_reward=207.87 +/- 0.00
Episode length: 762.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 762         |
|    mean_reward          | 208         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.014242296 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00174     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0113     |
|    std                  | 0.602       |
|    value_loss           | 0.00108     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 798      |
|    ep_rew_mean     | 170      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 643      |
|    iterations      | 33       |
|    time_elapsed    | 420      |
|    total_timesteps | 270336   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 794         |
|    ep_rew_mean          | 175         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 647         |
|    iterations           | 34          |
|    time_elapsed         | 429         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.016681686 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0143     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0137     |
|    std                  | 0.581       |
|    value_loss           | 0.00108     |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=225.17 +/- 0.00
Episode length: 802.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 802         |
|    mean_reward          | 225         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.014462735 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00285     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0123     |
|    std                  | 0.571       |
|    value_loss           | 0.0009      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 793      |
|    ep_rew_mean     | 182      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 648      |
|    iterations      | 35       |
|    time_elapsed    | 442      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=290000, episode_reward=237.64 +/- 0.00
Episode length: 848.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 848         |
|    mean_reward          | 238         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.017689083 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0135     |
|    std                  | 0.557       |
|    value_loss           | 0.000935    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 796      |
|    ep_rew_mean     | 187      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 649      |
|    iterations      | 36       |
|    time_elapsed    | 454      |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=300000, episode_reward=244.40 +/- 0.00
Episode length: 874.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 874         |
|    mean_reward          | 244         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.014134793 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0143      |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0111     |
|    std                  | 0.544       |
|    value_loss           | 0.000863    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 801      |
|    ep_rew_mean     | 193      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 647      |
|    iterations      | 37       |
|    time_elapsed    | 467      |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=310000, episode_reward=280.84 +/- 0.00
Episode length: 1149.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.15e+03   |
|    mean_reward          | 281        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 310000     |
| train/                  |            |
|    approx_kl            | 0.01492331 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.53      |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00834    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0107    |
|    std                  | 0.533      |
|    value_loss           | 0.00105    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 811      |
|    ep_rew_mean     | 200      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 646      |
|    iterations      | 38       |
|    time_elapsed    | 481      |
|    total_timesteps | 311296   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 830         |
|    ep_rew_mean          | 205         |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 650         |
|    iterations           | 39          |
|    time_elapsed         | 491         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.014559759 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0147     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00988    |
|    std                  | 0.527       |
|    value_loss           | 0.00101     |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=283.72 +/- 0.00
Episode length: 1073.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.07e+03    |
|    mean_reward          | 284         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.013515373 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00844     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00483    |
|    std                  | 0.519       |
|    value_loss           | 0.00154     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 851      |
|    ep_rew_mean     | 211      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 649      |
|    iterations      | 40       |
|    time_elapsed    | 504      |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=330000, episode_reward=283.30 +/- 0.00
Episode length: 1045.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.04e+03    |
|    mean_reward          | 283         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.012947589 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0195     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00602    |
|    std                  | 0.509       |
|    value_loss           | 0.00114     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 881      |
|    ep_rew_mean     | 220      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 648      |
|    iterations      | 41       |
|    time_elapsed    | 517      |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=340000, episode_reward=285.75 +/- 0.00
Episode length: 1023.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.02e+03    |
|    mean_reward          | 286         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.012748387 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000944   |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00694    |
|    std                  | 0.498       |
|    value_loss           | 0.000806    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 900      |
|    ep_rew_mean     | 226      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 645      |
|    iterations      | 42       |
|    time_elapsed    | 533      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=350000, episode_reward=287.01 +/- 0.00
Episode length: 1013.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.01e+03    |
|    mean_reward          | 287         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.013621412 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00466     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0102     |
|    std                  | 0.49        |
|    value_loss           | 0.000611    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 921      |
|    ep_rew_mean     | 232      |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 642      |
|    iterations      | 43       |
|    time_elapsed    | 548      |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=360000, episode_reward=290.35 +/- 0.00
Episode length: 999.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 999         |
|    mean_reward          | 290         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.012770253 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00752    |
|    std                  | 0.483       |
|    value_loss           | 0.000525    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 941      |
|    ep_rew_mean     | 239      |
|    success_rate    | 0.06     |
| time/              |          |
|    fps             | 639      |
|    iterations      | 44       |
|    time_elapsed    | 563      |
|    total_timesteps | 360448   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 936         |
|    ep_rew_mean          | 239         |
|    success_rate         | 0.14        |
| time/                   |             |
|    fps                  | 640         |
|    iterations           | 45          |
|    time_elapsed         | 575         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.014640183 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0367     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00861    |
|    std                  | 0.469       |
|    value_loss           | 0.000483    |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=288.93 +/- 0.00
Episode length: 978.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 978         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.014831916 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0266     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00371    |
|    std                  | 0.47        |
|    value_loss           | 0.0287      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 952      |
|    ep_rew_mean     | 245      |
|    success_rate    | 0.21     |
| time/              |          |
|    fps             | 638      |
|    iterations      | 46       |
|    time_elapsed    | 590      |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=380000, episode_reward=289.57 +/- 0.00
Episode length: 951.00 +/- 0.00
Success rate: 100.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 951          |
|    mean_reward          | 290          |
|    success_rate         | 1            |
| time/                   |              |
|    total_timesteps      | 380000       |
| train/                  |              |
|    approx_kl            | 0.0141956005 |
|    clip_fraction        | 0.166        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0246      |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00363     |
|    std                  | 0.456        |
|    value_loss           | 0.000935     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 966      |
|    ep_rew_mean     | 251      |
|    success_rate    | 0.3      |
| time/              |          |
|    fps             | 635      |
|    iterations      | 47       |
|    time_elapsed    | 605      |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=390000, episode_reward=288.62 +/- 0.00
Episode length: 930.00 +/- 0.00
Success rate: 100.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 930          |
|    mean_reward          | 289          |
|    success_rate         | 1            |
| time/                   |              |
|    total_timesteps      | 390000       |
| train/                  |              |
|    approx_kl            | 0.0154370675 |
|    clip_fraction        | 0.183        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.016       |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00899     |
|    std                  | 0.446        |
|    value_loss           | 0.000391     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 974      |
|    ep_rew_mean     | 257      |
|    success_rate    | 0.39     |
| time/              |          |
|    fps             | 633      |
|    iterations      | 48       |
|    time_elapsed    | 620      |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=400000, episode_reward=288.05 +/- 0.00
Episode length: 915.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 915         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.012953417 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00407    |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00867    |
|    std                  | 0.438       |
|    value_loss           | 0.000255    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 982      |
|    ep_rew_mean     | 262      |
|    success_rate    | 0.49     |
| time/              |          |
|    fps             | 633      |
|    iterations      | 49       |
|    time_elapsed    | 633      |
|    total_timesteps | 401408   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 984         |
|    ep_rew_mean          | 266         |
|    success_rate         | 0.57        |
| time/                   |             |
|    fps                  | 636         |
|    iterations           | 50          |
|    time_elapsed         | 643         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.015592819 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0114     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00821    |
|    std                  | 0.428       |
|    value_loss           | 0.000204    |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=288.53 +/- 0.00
Episode length: 884.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 884         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.012976572 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00412    |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00653    |
|    std                  | 0.42        |
|    value_loss           | 0.000256    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 268      |
|    success_rate    | 0.65     |
| time/              |          |
|    fps             | 637      |
|    iterations      | 51       |
|    time_elapsed    | 655      |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=420000, episode_reward=288.48 +/- 0.00
Episode length: 871.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 871         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.017304085 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0159     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00904    |
|    std                  | 0.407       |
|    value_loss           | 0.000221    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 951      |
|    ep_rew_mean     | 270      |
|    success_rate    | 0.75     |
| time/              |          |
|    fps             | 637      |
|    iterations      | 52       |
|    time_elapsed    | 668      |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=430000, episode_reward=288.17 +/- 0.00
Episode length: 856.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 856        |
|    mean_reward          | 288        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 430000     |
| train/                  |            |
|    approx_kl            | 0.01429251 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.962     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0275    |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.00989   |
|    std                  | 0.396      |
|    value_loss           | 0.000211   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 923      |
|    ep_rew_mean     | 269      |
|    success_rate    | 0.85     |
| time/              |          |
|    fps             | 637      |
|    iterations      | 53       |
|    time_elapsed    | 680      |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=440000, episode_reward=289.05 +/- 0.00
Episode length: 843.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 843         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.014410386 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.918      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0154      |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00404    |
|    std                  | 0.387       |
|    value_loss           | 0.0114      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 908      |
|    ep_rew_mean     | 270      |
|    success_rate    | 0.93     |
| time/              |          |
|    fps             | 637      |
|    iterations      | 54       |
|    time_elapsed    | 693      |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=450000, episode_reward=288.45 +/- 0.00
Episode length: 841.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 841         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.012931685 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.871      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00351    |
|    std                  | 0.377       |
|    value_loss           | 0.000257    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 890      |
|    ep_rew_mean     | 270      |
|    success_rate    | 0.95     |
| time/              |          |
|    fps             | 638      |
|    iterations      | 55       |
|    time_elapsed    | 705      |
|    total_timesteps | 450560   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 903         |
|    ep_rew_mean          | 279         |
|    success_rate         | 0.99        |
| time/                   |             |
|    fps                  | 641         |
|    iterations           | 56          |
|    time_elapsed         | 715         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.015102176 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.793      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0165     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0078     |
|    std                  | 0.361       |
|    value_loss           | 0.000179    |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=288.84 +/- 0.00
Episode length: 835.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 835         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.016340114 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0321      |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0069     |
|    std                  | 0.348       |
|    value_loss           | 0.000144    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 889      |
|    ep_rew_mean     | 279      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 641      |
|    iterations      | 57       |
|    time_elapsed    | 727      |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=470000, episode_reward=290.59 +/- 0.00
Episode length: 826.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 826         |
|    mean_reward          | 291         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.014495641 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.667      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0129     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00662    |
|    std                  | 0.339       |
|    value_loss           | 0.000122    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 875      |
|    ep_rew_mean     | 280      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 642      |
|    iterations      | 58       |
|    time_elapsed    | 739      |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=480000, episode_reward=291.12 +/- 0.00
Episode length: 820.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 820         |
|    mean_reward          | 291         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.018765902 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.628      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0275     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0076     |
|    std                  | 0.333       |
|    value_loss           | 0.000144    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 864      |
|    ep_rew_mean     | 280      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 642      |
|    iterations      | 59       |
|    time_elapsed    | 751      |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=490000, episode_reward=291.98 +/- 0.00
Episode length: 819.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 819         |
|    mean_reward          | 292         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.015221717 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.573      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0247      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00799    |
|    std                  | 0.324       |
|    value_loss           | 0.000119    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 845      |
|    ep_rew_mean     | 278      |
|    success_rate    | 0.98     |
| time/              |          |
|    fps             | 642      |
|    iterations      | 60       |
|    time_elapsed    | 764      |
|    total_timesteps | 491520   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 838         |
|    ep_rew_mean          | 278         |
|    success_rate         | 0.98        |
| time/                   |             |
|    fps                  | 645         |
|    iterations           | 61          |
|    time_elapsed         | 774         |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.014647657 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00357    |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00484    |
|    std                  | 0.319       |
|    value_loss           | 0.0108      |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=292.55 +/- 0.00
Episode length: 810.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 810         |
|    mean_reward          | 293         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.015113367 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.48       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0357      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00175    |
|    std                  | 0.307       |
|    value_loss           | 0.000247    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 820      |
|    ep_rew_mean     | 276      |
|    success_rate    | 0.97     |
| time/              |          |
|    fps             | 645      |
|    iterations      | 62       |
|    time_elapsed    | 786      |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=510000, episode_reward=275.07 +/- 0.00
Episode length: 731.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 731         |
|    mean_reward          | 275         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.016546812 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.417      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0236      |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.000346   |
|    std                  | 0.3         |
|    value_loss           | 0.0103      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 822      |
|    ep_rew_mean     | 278      |
|    success_rate    | 0.96     |
| time/              |          |
|    fps             | 646      |
|    iterations      | 63       |
|    time_elapsed    | 798      |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=520000, episode_reward=274.99 +/- 0.00
Episode length: 728.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 728         |
|    mean_reward          | 275         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 520000      |
| train/                  |             |
|    approx_kl            | 0.016498309 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.365      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0318     |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.000161   |
|    std                  | 0.291       |
|    value_loss           | 0.00178     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 815      |
|    ep_rew_mean     | 278      |
|    success_rate    | 0.94     |
| time/              |          |
|    fps             | 646      |
|    iterations      | 64       |
|    time_elapsed    | 810      |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=530000, episode_reward=275.32 +/- 0.00
Episode length: 722.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 722         |
|    mean_reward          | 275         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 530000      |
| train/                  |             |
|    approx_kl            | 0.015458517 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.335      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.021       |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.000217   |
|    std                  | 0.289       |
|    value_loss           | 0.00152     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 798      |
|    ep_rew_mean     | 275      |
|    success_rate    | 0.88     |
| time/              |          |
|    fps             | 647      |
|    iterations      | 65       |
|    time_elapsed    | 822      |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=540000, episode_reward=292.47 +/- 0.00
Episode length: 810.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 810         |
|    mean_reward          | 292         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 540000      |
| train/                  |             |
|    approx_kl            | 0.015590852 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0172     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.000708   |
|    std                  | 0.294       |
|    value_loss           | 0.0125      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 794      |
|    ep_rew_mean     | 275      |
|    success_rate    | 0.87     |
| time/              |          |
|    fps             | 647      |
|    iterations      | 66       |
|    time_elapsed    | 835      |
|    total_timesteps | 540672   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 793         |
|    ep_rew_mean          | 275         |
|    success_rate         | 0.87        |
| time/                   |             |
|    fps                  | 649         |
|    iterations           | 67          |
|    time_elapsed         | 844         |
|    total_timesteps      | 548864      |
| train/                  |             |
|    approx_kl            | 0.015289716 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.341      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00539    |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.000524   |
|    std                  | 0.29        |
|    value_loss           | 0.000995    |
-----------------------------------------
Eval num_timesteps=550000, episode_reward=293.93 +/- 0.00
Episode length: 824.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 824         |
|    mean_reward          | 294         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.016442662 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.287      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00195    |
|    std                  | 0.281       |
|    value_loss           | 0.000199    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 792      |
|    ep_rew_mean     | 275      |
|    success_rate    | 0.87     |
| time/              |          |
|    fps             | 649      |
|    iterations      | 68       |
|    time_elapsed    | 857      |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=560000, episode_reward=294.69 +/- 0.00
Episode length: 821.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 821         |
|    mean_reward          | 295         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.014119746 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0294     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.000317   |
|    std                  | 0.277       |
|    value_loss           | 0.000113    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 791      |
|    ep_rew_mean     | 275      |
|    success_rate    | 0.87     |
| time/              |          |
|    fps             | 649      |
|    iterations      | 69       |
|    time_elapsed    | 869      |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=570000, episode_reward=294.20 +/- 0.00
Episode length: 815.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 815         |
|    mean_reward          | 294         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 570000      |
| train/                  |             |
|    approx_kl            | 0.019372635 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0336      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00255    |
|    std                  | 0.268       |
|    value_loss           | 0.000109    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 799      |
|    ep_rew_mean     | 278      |
|    success_rate    | 0.88     |
| time/              |          |
|    fps             | 648      |
|    iterations      | 70       |
|    time_elapsed    | 884      |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=580000, episode_reward=293.60 +/- 0.00
Episode length: 810.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 810        |
|    mean_reward          | 294        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 580000     |
| train/                  |            |
|    approx_kl            | 0.01669997 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.134     |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0146     |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.000889  |
|    std                  | 0.26       |
|    value_loss           | 7.16e-05   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 807      |
|    ep_rew_mean     | 281      |
|    success_rate    | 0.89     |
| time/              |          |
|    fps             | 648      |
|    iterations      | 71       |
|    time_elapsed    | 896      |
|    total_timesteps | 581632   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 807         |
|    ep_rew_mean          | 281         |
|    success_rate         | 0.89        |
| time/                   |             |
|    fps                  | 650         |
|    iterations           | 72          |
|    time_elapsed         | 906         |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.017470907 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0679     |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0192     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00257    |
|    std                  | 0.252       |
|    value_loss           | 9.34e-05    |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=295.23 +/- 0.00
Episode length: 808.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 808         |
|    mean_reward          | 295         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.018069524 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00244    |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0242     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0035     |
|    std                  | 0.244       |
|    value_loss           | 6.37e-05    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 808      |
|    ep_rew_mean     | 281      |
|    success_rate    | 0.91     |
| time/              |          |
|    fps             | 650      |
|    iterations      | 73       |
|    time_elapsed    | 919      |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=600000, episode_reward=295.08 +/- 0.00
Episode length: 805.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 805         |
|    mean_reward          | 295         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.018371988 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.0588      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00788    |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00106    |
|    std                  | 0.237       |
|    value_loss           | 0.000102    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 774      |
|    ep_rew_mean     | 269      |
|    success_rate    | 0.91     |
| time/              |          |
|    fps             | 650      |
|    iterations      | 74       |
|    time_elapsed    | 931      |
|    total_timesteps | 606208   |
---------------------------------
Modele final sauvegarde: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/final_model.zip
Meilleur modele sauvegarde: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/best_model.zip
Statistiques VecNormalize: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/vecnormalize.pkl
