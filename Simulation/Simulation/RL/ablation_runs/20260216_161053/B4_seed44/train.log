Using cpu device
Debut de l'entrainement...
Logging to /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/logs/tensorboard/PPO_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 265      |
|    ep_rew_mean     | -19.1    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 1741     |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=6.02 +/- 0.00
Episode length: 287.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 287         |
|    mean_reward          | 6.02        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011043805 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0196      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00756    |
|    std                  | 0.98        |
|    value_loss           | 0.13        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | -16.2    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 904      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=9.04 +/- 0.00
Episode length: 320.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 320          |
|    mean_reward          | 9.04         |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0142879775 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.62         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00734     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0156      |
|    std                  | 0.987        |
|    value_loss           | 0.00788      |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | -14.3    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 779      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=30000, episode_reward=30.89 +/- 0.00
Episode length: 646.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 646         |
|    mean_reward          | 30.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.012503991 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.82       |
|    explained_variance   | 0.549       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0356     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0163     |
|    std                  | 0.99        |
|    value_loss           | 0.0119      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 336      |
|    ep_rew_mean     | -12.7    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 716      |
|    iterations      | 4        |
|    time_elapsed    | 45       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=40000, episode_reward=87.37 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 87.4        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.015416256 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0308     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0157     |
|    std                  | 0.984       |
|    value_loss           | 0.00932     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 424      |
|    ep_rew_mean     | -15.4    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 654      |
|    iterations      | 5        |
|    time_elapsed    | 62       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 487         |
|    ep_rew_mean          | -14.6       |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 675         |
|    iterations           | 6           |
|    time_elapsed         | 72          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.009738822 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.78       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00845    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0093     |
|    std                  | 0.973       |
|    value_loss           | 0.00742     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=79.72 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 79.7        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.018643152 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.77       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0284     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0277     |
|    std                  | 0.969       |
|    value_loss           | 0.0044      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 539      |
|    ep_rew_mean     | -13.7    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 645      |
|    iterations      | 7        |
|    time_elapsed    | 88       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=82.04 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 82          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.018412778 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00991    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0247     |
|    std                  | 0.965       |
|    value_loss           | 0.00418     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 589      |
|    ep_rew_mean     | -11.4    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 609      |
|    iterations      | 8        |
|    time_elapsed    | 107      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=70000, episode_reward=12.43 +/- 0.00
Episode length: 351.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | 12.4        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.012542145 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.018       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.017      |
|    std                  | 0.958       |
|    value_loss           | 0.00579     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 624      |
|    ep_rew_mean     | -6.77    |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 610      |
|    iterations      | 9        |
|    time_elapsed    | 120      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=80000, episode_reward=8.85 +/- 0.00
Episode length: 298.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 298         |
|    mean_reward          | 8.85        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.012300983 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0225     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.012      |
|    std                  | 0.947       |
|    value_loss           | 0.0134      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 642      |
|    ep_rew_mean     | -1.8     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 612      |
|    iterations      | 10       |
|    time_elapsed    | 133      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=90000, episode_reward=8.59 +/- 0.00
Episode length: 270.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | 8.59        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.012379698 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0242      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0103     |
|    std                  | 0.931       |
|    value_loss           | 0.0117      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 595      |
|    ep_rew_mean     | 3.17     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 620      |
|    iterations      | 11       |
|    time_elapsed    | 145      |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 271         |
|    ep_rew_mean          | 6.93        |
|    success_rate         | 0           |
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 12          |
|    time_elapsed         | 157         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.012709717 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00548    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0116     |
|    std                  | 0.918       |
|    value_loss           | 0.0138      |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=10.04 +/- 0.00
Episode length: 245.00 +/- 0.00
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 245       |
|    mean_reward          | 10        |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 100000    |
| train/                  |           |
|    approx_kl            | 0.0106943 |
|    clip_fraction        | 0.142     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.61     |
|    explained_variance   | 0.888     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.000718  |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.00939  |
|    std                  | 0.91      |
|    value_loss           | 0.0117    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 6.66     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 622      |
|    iterations      | 13       |
|    time_elapsed    | 170      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=110000, episode_reward=89.96 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 90          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.011503875 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00528     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0124     |
|    std                  | 0.897       |
|    value_loss           | 0.0116      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 9.21     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 614      |
|    iterations      | 14       |
|    time_elapsed    | 186      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=120000, episode_reward=97.95 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 97.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.012957204 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0293     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0141     |
|    std                  | 0.889       |
|    value_loss           | 0.00719     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | 12       |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 603      |
|    iterations      | 15       |
|    time_elapsed    | 203      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=130000, episode_reward=152.72 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 153        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.01468127 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.55      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.032     |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0165    |
|    std                  | 0.899      |
|    value_loss           | 0.0114     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 411      |
|    ep_rew_mean     | 15.7     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 596      |
|    iterations      | 16       |
|    time_elapsed    | 219      |
|    total_timesteps | 131072   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 484        |
|    ep_rew_mean          | 20.1       |
|    success_rate         | 0          |
| time/                   |            |
|    fps                  | 606        |
|    iterations           | 17         |
|    time_elapsed         | 229        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.01758138 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.53      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0198    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0176    |
|    std                  | 0.898      |
|    value_loss           | 0.00666    |
----------------------------------------
Eval num_timesteps=140000, episode_reward=199.28 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 199         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.015805354 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0124     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0164     |
|    std                  | 0.904       |
|    value_loss           | 0.00894     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 539      |
|    ep_rew_mean     | 25       |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 601      |
|    iterations      | 18       |
|    time_elapsed    | 245      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=223.05 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 223        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.01540966 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.51      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.03      |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0175    |
|    std                  | 0.894      |
|    value_loss           | 0.00693    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 612      |
|    ep_rew_mean     | 31.2     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 595      |
|    iterations      | 19       |
|    time_elapsed    | 261      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=160000, episode_reward=239.78 +/- 0.00
Episode length: 2000.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2e+03        |
|    mean_reward          | 240          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0148751605 |
|    clip_fraction        | 0.204        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.49        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000725     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.0149      |
|    std                  | 0.886        |
|    value_loss           | 0.0105       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 684      |
|    ep_rew_mean     | 38.1     |
|    success_rate    | 0        |
| time/              |          |
|    fps             | 589      |
|    iterations      | 20       |
|    time_elapsed    | 277      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=170000, episode_reward=257.05 +/- 0.00
Episode length: 1973.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.97e+03    |
|    mean_reward          | 257         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.013184398 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00407    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.014      |
|    std                  | 0.888       |
|    value_loss           | 0.011       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 755      |
|    ep_rew_mean     | 45.5     |
|    success_rate    | 0.01     |
| time/              |          |
|    fps             | 580      |
|    iterations      | 21       |
|    time_elapsed    | 296      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=180000, episode_reward=259.66 +/- 0.00
Episode length: 1823.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.82e+03    |
|    mean_reward          | 260         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.013927219 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00651    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0132     |
|    std                  | 0.873       |
|    value_loss           | 0.0199      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 823      |
|    ep_rew_mean     | 53.4     |
|    success_rate    | 0.05     |
| time/              |          |
|    fps             | 573      |
|    iterations      | 22       |
|    time_elapsed    | 314      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 932         |
|    ep_rew_mean          | 67.6        |
|    success_rate         | 0.11        |
| time/                   |             |
|    fps                  | 577         |
|    iterations           | 23          |
|    time_elapsed         | 326         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.016797945 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00869    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0173     |
|    std                  | 0.858       |
|    value_loss           | 0.00612     |
-----------------------------------------
Eval num_timesteps=190000, episode_reward=264.35 +/- 0.00
Episode length: 1566.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.57e+03    |
|    mean_reward          | 264         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 0.015862402 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00597     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0171     |
|    std                  | 0.843       |
|    value_loss           | 0.00603     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 988      |
|    ep_rew_mean     | 76       |
|    success_rate    | 0.14     |
| time/              |          |
|    fps             | 573      |
|    iterations      | 24       |
|    time_elapsed    | 343      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=268.37 +/- 0.00
Episode length: 1475.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.48e+03   |
|    mean_reward          | 268        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 200000     |
| train/                  |            |
|    approx_kl            | 0.01875019 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.35      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00173    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0173    |
|    std                  | 0.821      |
|    value_loss           | 0.0119     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.06e+03 |
|    ep_rew_mean     | 86.9     |
|    success_rate    | 0.19     |
| time/              |          |
|    fps             | 570      |
|    iterations      | 25       |
|    time_elapsed    | 358      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=210000, episode_reward=270.03 +/- 0.00
Episode length: 1389.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.39e+03    |
|    mean_reward          | 270         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.016380623 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0531     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0173     |
|    std                  | 0.802       |
|    value_loss           | 0.00399     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | 102      |
|    success_rate    | 0.24     |
| time/              |          |
|    fps             | 570      |
|    iterations      | 26       |
|    time_elapsed    | 373      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=220000, episode_reward=271.28 +/- 0.00
Episode length: 1320.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.32e+03    |
|    mean_reward          | 271         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.019293856 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0184     |
|    std                  | 0.782       |
|    value_loss           | 0.00678     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.19e+03 |
|    ep_rew_mean     | 113      |
|    success_rate    | 0.29     |
| time/              |          |
|    fps             | 570      |
|    iterations      | 27       |
|    time_elapsed    | 387      |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.28e+03    |
|    ep_rew_mean          | 131         |
|    success_rate         | 0.35        |
| time/                   |             |
|    fps                  | 576         |
|    iterations           | 28          |
|    time_elapsed         | 397         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.018453136 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0264     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0198     |
|    std                  | 0.76        |
|    value_loss           | 0.00723     |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=277.98 +/- 0.00
Episode length: 1184.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.18e+03    |
|    mean_reward          | 278         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.018398749 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.025      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0167     |
|    std                  | 0.74        |
|    value_loss           | 0.00534     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.34e+03 |
|    ep_rew_mean     | 146      |
|    success_rate    | 0.41     |
| time/              |          |
|    fps             | 577      |
|    iterations      | 29       |
|    time_elapsed    | 411      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=240000, episode_reward=279.67 +/- 0.00
Episode length: 1127.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | 280         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.018658482 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.049      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0166     |
|    std                  | 0.715       |
|    value_loss           | 0.00215     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.36e+03 |
|    ep_rew_mean     | 161      |
|    success_rate    | 0.47     |
| time/              |          |
|    fps             | 579      |
|    iterations      | 30       |
|    time_elapsed    | 424      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=250000, episode_reward=281.21 +/- 0.00
Episode length: 1080.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.08e+03    |
|    mean_reward          | 281         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.018560372 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0215     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0172     |
|    std                  | 0.687       |
|    value_loss           | 0.00338     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.35e+03 |
|    ep_rew_mean     | 175      |
|    success_rate    | 0.54     |
| time/              |          |
|    fps             | 580      |
|    iterations      | 31       |
|    time_elapsed    | 437      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=260000, episode_reward=280.31 +/- 0.00
Episode length: 1035.00 +/- 0.00
Success rate: 100.00%
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.04e+03 |
|    mean_reward          | 280      |
|    success_rate         | 1        |
| time/                   |          |
|    total_timesteps      | 260000   |
| train/                  |          |
|    approx_kl            | 0.016778 |
|    clip_fraction        | 0.215    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.98    |
|    explained_variance   | 0.993    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.022   |
|    n_updates            | 310      |
|    policy_gradient_loss | -0.0166  |
|    std                  | 0.665    |
|    value_loss           | 0.00171  |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.33e+03 |
|    ep_rew_mean     | 190      |
|    success_rate    | 0.6      |
| time/              |          |
|    fps             | 581      |
|    iterations      | 32       |
|    time_elapsed    | 450      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=270000, episode_reward=281.04 +/- 0.00
Episode length: 1004.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 281         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.016695261 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0432     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0129     |
|    std                  | 0.646       |
|    value_loss           | 0.0035      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.29e+03 |
|    ep_rew_mean     | 203      |
|    success_rate    | 0.66     |
| time/              |          |
|    fps             | 583      |
|    iterations      | 33       |
|    time_elapsed    | 463      |
|    total_timesteps | 270336   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.23e+03    |
|    ep_rew_mean          | 212         |
|    success_rate         | 0.74        |
| time/                   |             |
|    fps                  | 588         |
|    iterations           | 34          |
|    time_elapsed         | 472         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.014407257 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00284    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0119     |
|    std                  | 0.633       |
|    value_loss           | 0.0032      |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=283.16 +/- 0.00
Episode length: 977.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 977         |
|    mean_reward          | 283         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.015384496 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0126      |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0105     |
|    std                  | 0.615       |
|    value_loss           | 0.00105     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.21e+03 |
|    ep_rew_mean     | 224      |
|    success_rate    | 0.82     |
| time/              |          |
|    fps             | 590      |
|    iterations      | 35       |
|    time_elapsed    | 485      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=290000, episode_reward=284.27 +/- 0.00
Episode length: 938.00 +/- 0.00
Success rate: 100.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 938          |
|    mean_reward          | 284          |
|    success_rate         | 1            |
| time/                   |              |
|    total_timesteps      | 290000       |
| train/                  |              |
|    approx_kl            | 0.0138517935 |
|    clip_fraction        | 0.179        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0237      |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.0105      |
|    std                  | 0.602        |
|    value_loss           | 0.000815     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.18e+03 |
|    ep_rew_mean     | 235      |
|    success_rate    | 0.86     |
| time/              |          |
|    fps             | 591      |
|    iterations      | 36       |
|    time_elapsed    | 498      |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=300000, episode_reward=283.63 +/- 0.00
Episode length: 917.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 917         |
|    mean_reward          | 284         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.015834047 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.015       |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0103     |
|    std                  | 0.584       |
|    value_loss           | 0.000783    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | 242      |
|    success_rate    | 0.88     |
| time/              |          |
|    fps             | 593      |
|    iterations      | 37       |
|    time_elapsed    | 510      |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=310000, episode_reward=285.11 +/- 0.00
Episode length: 893.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 893         |
|    mean_reward          | 285         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.013167561 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0141     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00921    |
|    std                  | 0.568       |
|    value_loss           | 0.000561    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.05e+03 |
|    ep_rew_mean     | 244      |
|    success_rate    | 0.87     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 38       |
|    time_elapsed    | 523      |
|    total_timesteps | 311296   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 247         |
|    success_rate         | 0.89        |
| time/                   |             |
|    fps                  | 599         |
|    iterations           | 39          |
|    time_elapsed         | 533         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.011050168 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0323      |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00444    |
|    std                  | 0.56        |
|    value_loss           | 0.0212      |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=286.58 +/- 0.00
Episode length: 861.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 861         |
|    mean_reward          | 287         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.013019908 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0251     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00432    |
|    std                  | 0.546       |
|    value_loss           | 0.0107      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | 249      |
|    success_rate    | 0.89     |
| time/              |          |
|    fps             | 600      |
|    iterations      | 40       |
|    time_elapsed    | 545      |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=330000, episode_reward=288.17 +/- 0.00
Episode length: 862.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 862         |
|    mean_reward          | 288         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.017649911 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.01        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0093     |
|    std                  | 0.523       |
|    value_loss           | 0.00138     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 252      |
|    success_rate    | 0.91     |
| time/              |          |
|    fps             | 602      |
|    iterations      | 41       |
|    time_elapsed    | 557      |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=340000, episode_reward=286.04 +/- 0.00
Episode length: 848.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 848         |
|    mean_reward          | 286         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.015276024 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00362     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0107     |
|    std                  | 0.508       |
|    value_loss           | 0.000427    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 931      |
|    ep_rew_mean     | 254      |
|    success_rate    | 0.91     |
| time/              |          |
|    fps             | 603      |
|    iterations      | 42       |
|    time_elapsed    | 569      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=350000, episode_reward=286.03 +/- 0.00
Episode length: 844.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 844         |
|    mean_reward          | 286         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.017505445 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0141     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0148     |
|    std                  | 0.484       |
|    value_loss           | 0.000358    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 916      |
|    ep_rew_mean     | 255      |
|    success_rate    | 0.93     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 43       |
|    time_elapsed    | 582      |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=360000, episode_reward=287.15 +/- 0.00
Episode length: 842.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 842         |
|    mean_reward          | 287         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.018247262 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0482     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0119     |
|    std                  | 0.463       |
|    value_loss           | 0.0003      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 900      |
|    ep_rew_mean     | 257      |
|    success_rate    | 0.95     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 44       |
|    time_elapsed    | 594      |
|    total_timesteps | 360448   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 884         |
|    ep_rew_mean          | 258         |
|    success_rate         | 0.95        |
| time/                   |             |
|    fps                  | 610         |
|    iterations           | 45          |
|    time_elapsed         | 603         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.017041981 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00817     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0113     |
|    std                  | 0.445       |
|    value_loss           | 0.000243    |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=289.55 +/- 0.00
Episode length: 828.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 828         |
|    mean_reward          | 290         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.016538661 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0162     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00974    |
|    std                  | 0.425       |
|    value_loss           | 0.00023     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 862      |
|    ep_rew_mean     | 257      |
|    success_rate    | 0.94     |
| time/              |          |
|    fps             | 611      |
|    iterations      | 46       |
|    time_elapsed    | 616      |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=380000, episode_reward=289.37 +/- 0.00
Episode length: 821.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 821         |
|    mean_reward          | 289         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 380000      |
| train/                  |             |
|    approx_kl            | 0.016100287 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00589    |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00557    |
|    std                  | 0.412       |
|    value_loss           | 0.0104      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 849      |
|    ep_rew_mean     | 258      |
|    success_rate    | 0.94     |
| time/              |          |
|    fps             | 612      |
|    iterations      | 47       |
|    time_elapsed    | 628      |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=390000, episode_reward=290.32 +/- 0.00
Episode length: 817.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 817         |
|    mean_reward          | 290         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.015347753 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.975      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0127      |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00375    |
|    std                  | 0.395       |
|    value_loss           | 0.000366    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 851      |
|    ep_rew_mean     | 262      |
|    success_rate    | 0.96     |
| time/              |          |
|    fps             | 614      |
|    iterations      | 48       |
|    time_elapsed    | 640      |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=400000, episode_reward=291.85 +/- 0.00
Episode length: 810.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 810         |
|    mean_reward          | 292         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.016028589 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.893      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0143      |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00493    |
|    std                  | 0.379       |
|    value_loss           | 0.000212    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 857      |
|    ep_rew_mean     | 268      |
|    success_rate    | 0.98     |
| time/              |          |
|    fps             | 615      |
|    iterations      | 49       |
|    time_elapsed    | 652      |
|    total_timesteps | 401408   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 851         |
|    ep_rew_mean          | 269         |
|    success_rate         | 0.99        |
| time/                   |             |
|    fps                  | 618         |
|    iterations           | 50          |
|    time_elapsed         | 662         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.015488118 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.806      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00971     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00628    |
|    std                  | 0.363       |
|    value_loss           | 0.000156    |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=292.82 +/- 0.00
Episode length: 800.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 800        |
|    mean_reward          | 293        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 410000     |
| train/                  |            |
|    approx_kl            | 0.01497704 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0303    |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.00437   |
|    std                  | 0.349      |
|    value_loss           | 0.000157   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 842      |
|    ep_rew_mean     | 271      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 619      |
|    iterations      | 51       |
|    time_elapsed    | 674      |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=420000, episode_reward=294.00 +/- 0.00
Episode length: 799.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 799        |
|    mean_reward          | 294        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.01814475 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.627     |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00751   |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.00725   |
|    std                  | 0.333      |
|    value_loss           | 0.000114   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 834      |
|    ep_rew_mean     | 272      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 620      |
|    iterations      | 52       |
|    time_elapsed    | 687      |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=430000, episode_reward=293.54 +/- 0.00
Episode length: 793.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 793         |
|    mean_reward          | 294         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.015933536 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.54       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0134     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00763    |
|    std                  | 0.319       |
|    value_loss           | 0.000158    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 826      |
|    ep_rew_mean     | 273      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 620      |
|    iterations      | 53       |
|    time_elapsed    | 699      |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=440000, episode_reward=293.74 +/- 0.00
Episode length: 791.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 791         |
|    mean_reward          | 294         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.017070033 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.468      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0264     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00468    |
|    std                  | 0.308       |
|    value_loss           | 0.000183    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 821      |
|    ep_rew_mean     | 274      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 621      |
|    iterations      | 54       |
|    time_elapsed    | 711      |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=450000, episode_reward=293.89 +/- 0.00
Episode length: 789.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 789        |
|    mean_reward          | 294        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 450000     |
| train/                  |            |
|    approx_kl            | 0.01855023 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.394     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0135    |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.00506   |
|    std                  | 0.297      |
|    value_loss           | 0.000255   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 815      |
|    ep_rew_mean     | 275      |
|    success_rate    | 0.99     |
| time/              |          |
|    fps             | 622      |
|    iterations      | 55       |
|    time_elapsed    | 723      |
|    total_timesteps | 450560   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 818        |
|    ep_rew_mean          | 278        |
|    success_rate         | 1          |
| time/                   |            |
|    fps                  | 625        |
|    iterations           | 56         |
|    time_elapsed         | 733        |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.01792524 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.341     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0159     |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.00262   |
|    std                  | 0.291      |
|    value_loss           | 0.000295   |
----------------------------------------
Eval num_timesteps=460000, episode_reward=293.39 +/- 0.00
Episode length: 784.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 784         |
|    mean_reward          | 293         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.013965655 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.278      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0175     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00254    |
|    std                  | 0.282       |
|    value_loss           | 0.000158    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 812      |
|    ep_rew_mean     | 279      |
|    success_rate    | 0.98     |
| time/              |          |
|    fps             | 626      |
|    iterations      | 57       |
|    time_elapsed    | 745      |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=470000, episode_reward=293.22 +/- 0.00
Episode length: 782.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 782         |
|    mean_reward          | 293         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.015110919 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.228      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0259     |
|    n_updates            | 570         |
|    policy_gradient_loss | 0.00102     |
|    std                  | 0.276       |
|    value_loss           | 0.0017      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 797      |
|    ep_rew_mean     | 276      |
|    success_rate    | 0.93     |
| time/              |          |
|    fps             | 627      |
|    iterations      | 58       |
|    time_elapsed    | 757      |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=480000, episode_reward=293.37 +/- 0.00
Episode length: 786.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 786         |
|    mean_reward          | 293         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.015294488 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.247      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000254   |
|    n_updates            | 580         |
|    policy_gradient_loss | 0.00117     |
|    std                  | 0.283       |
|    value_loss           | 0.0129      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 780      |
|    ep_rew_mean     | 271      |
|    success_rate    | 0.91     |
| time/              |          |
|    fps             | 627      |
|    iterations      | 59       |
|    time_elapsed    | 769      |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=490000, episode_reward=293.30 +/- 0.00
Episode length: 789.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 789         |
|    mean_reward          | 293         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.013850171 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.256      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00995    |
|    n_updates            | 590         |
|    policy_gradient_loss | 0.000204    |
|    std                  | 0.282       |
|    value_loss           | 0.0114      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 777      |
|    ep_rew_mean     | 272      |
|    success_rate    | 0.91     |
| time/              |          |
|    fps             | 628      |
|    iterations      | 60       |
|    time_elapsed    | 782      |
|    total_timesteps | 491520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 775        |
|    ep_rew_mean          | 272        |
|    success_rate         | 0.91       |
| time/                   |            |
|    fps                  | 630        |
|    iterations           | 61         |
|    time_elapsed         | 792        |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.02010993 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.222     |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0214    |
|    n_updates            | 600        |
|    policy_gradient_loss | 0.00121    |
|    std                  | 0.275      |
|    value_loss           | 0.000299   |
----------------------------------------
Eval num_timesteps=500000, episode_reward=293.70 +/- 0.00
Episode length: 794.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 794         |
|    mean_reward          | 294         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.016178135 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00488     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.000921   |
|    std                  | 0.265       |
|    value_loss           | 0.000155    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 774      |
|    ep_rew_mean     | 272      |
|    success_rate    | 0.91     |
| time/              |          |
|    fps             | 631      |
|    iterations      | 62       |
|    time_elapsed    | 804      |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=510000, episode_reward=293.11 +/- 0.00
Episode length: 788.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 788        |
|    mean_reward          | 293        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 510000     |
| train/                  |            |
|    approx_kl            | 0.01672658 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0711    |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0112    |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.00297   |
|    std                  | 0.255      |
|    value_loss           | 8.49e-05   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 773      |
|    ep_rew_mean     | 273      |
|    success_rate    | 0.91     |
| time/              |          |
|    fps             | 632      |
|    iterations      | 63       |
|    time_elapsed    | 816      |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=520000, episode_reward=293.03 +/- 0.00
Episode length: 786.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 786        |
|    mean_reward          | 293        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.02183104 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.0332     |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0177     |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.00311   |
|    std                  | 0.242      |
|    value_loss           | 7.49e-05   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 772      |
|    ep_rew_mean     | 273      |
|    success_rate    | 0.91     |
| time/              |          |
|    fps             | 632      |
|    iterations      | 64       |
|    time_elapsed    | 828      |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=530000, episode_reward=13.84 +/- 0.00
Episode length: 56.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56          |
|    mean_reward          | 13.8        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 530000      |
| train/                  |             |
|    approx_kl            | 0.018240858 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.122       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0177     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00545    |
|    std                  | 0.233       |
|    value_loss           | 6.21e-05    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 711      |
|    ep_rew_mean     | 252      |
|    success_rate    | 0.83     |
| time/              |          |
|    fps             | 634      |
|    iterations      | 65       |
|    time_elapsed    | 838      |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=540000, episode_reward=294.67 +/- 0.00
Episode length: 788.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 788        |
|    mean_reward          | 295        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.01952738 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.0828     |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.064      |
|    n_updates            | 650        |
|    policy_gradient_loss | 0.00378    |
|    std                  | 0.244      |
|    value_loss           | 0.0439     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 712      |
|    ep_rew_mean     | 253      |
|    success_rate    | 0.85     |
| time/              |          |
|    fps             | 635      |
|    iterations      | 66       |
|    time_elapsed    | 850      |
|    total_timesteps | 540672   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 721        |
|    ep_rew_mean          | 256        |
|    success_rate         | 0.9        |
| time/                   |            |
|    fps                  | 637        |
|    iterations           | 67         |
|    time_elapsed         | 860        |
|    total_timesteps      | 548864     |
| train/                  |            |
|    approx_kl            | 0.02110847 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.128      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00488    |
|    n_updates            | 660        |
|    policy_gradient_loss | 0.0018     |
|    std                  | 0.231      |
|    value_loss           | 0.000532   |
----------------------------------------
Eval num_timesteps=550000, episode_reward=13.90 +/- 0.00
Episode length: 53.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53          |
|    mean_reward          | 13.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.021374632 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.184       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0226     |
|    n_updates            | 670         |
|    policy_gradient_loss | 0.00254     |
|    std                  | 0.228       |
|    value_loss           | 0.0165      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 646      |
|    ep_rew_mean     | 230      |
|    success_rate    | 0.8      |
| time/              |          |
|    fps             | 639      |
|    iterations      | 68       |
|    time_elapsed    | 870      |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=560000, episode_reward=294.40 +/- 0.00
Episode length: 787.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 787         |
|    mean_reward          | 294         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.019670852 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.143       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.044      |
|    n_updates            | 680         |
|    policy_gradient_loss | 0.00587     |
|    std                  | 0.237       |
|    value_loss           | 0.0455      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 645      |
|    ep_rew_mean     | 230      |
|    success_rate    | 0.8      |
| time/              |          |
|    fps             | 640      |
|    iterations      | 69       |
|    time_elapsed    | 882      |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=570000, episode_reward=294.35 +/- 0.00
Episode length: 788.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 788         |
|    mean_reward          | 294         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 570000      |
| train/                  |             |
|    approx_kl            | 0.017159767 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.161       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0156      |
|    n_updates            | 690         |
|    policy_gradient_loss | 0.00426     |
|    std                  | 0.229       |
|    value_loss           | 0.000913    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 592      |
|    ep_rew_mean     | 211      |
|    success_rate    | 0.73     |
| time/              |          |
|    fps             | 640      |
|    iterations      | 70       |
|    time_elapsed    | 894      |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=580000, episode_reward=294.02 +/- 0.00
Episode length: 786.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 786         |
|    mean_reward          | 294         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.022415072 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.136       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0203      |
|    n_updates            | 700         |
|    policy_gradient_loss | 0.00398     |
|    std                  | 0.238       |
|    value_loss           | 0.0366      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 592      |
|    ep_rew_mean     | 212      |
|    success_rate    | 0.73     |
| time/              |          |
|    fps             | 641      |
|    iterations      | 71       |
|    time_elapsed    | 907      |
|    total_timesteps | 581632   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 622         |
|    ep_rew_mean          | 222         |
|    success_rate         | 0.77        |
| time/                   |             |
|    fps                  | 643         |
|    iterations           | 72          |
|    time_elapsed         | 916         |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.025664572 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.168       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0298     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.000401   |
|    std                  | 0.229       |
|    value_loss           | 0.00157     |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=294.04 +/- 0.00
Episode length: 786.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 786         |
|    mean_reward          | 294         |
|    success_rate         | 1           |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.018254787 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | 0.23        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0143     |
|    n_updates            | 720         |
|    policy_gradient_loss | 0.00294     |
|    std                  | 0.223       |
|    value_loss           | 0.000683    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 652      |
|    ep_rew_mean     | 233      |
|    success_rate    | 0.81     |
| time/              |          |
|    fps             | 643      |
|    iterations      | 73       |
|    time_elapsed    | 929      |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=600000, episode_reward=294.48 +/- 0.00
Episode length: 782.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 782        |
|    mean_reward          | 294        |
|    success_rate         | 1          |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.02126078 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.283      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.02       |
|    n_updates            | 730        |
|    policy_gradient_loss | 0.00391    |
|    std                  | 0.218      |
|    value_loss           | 0.000281   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 652      |
|    ep_rew_mean     | 233      |
|    success_rate    | 0.81     |
| time/              |          |
|    fps             | 643      |
|    iterations      | 74       |
|    time_elapsed    | 941      |
|    total_timesteps | 606208   |
---------------------------------
Modele final sauvegarde: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/final_model.zip
Meilleur modele sauvegarde: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/best_model.zip
Statistiques VecNormalize: /home/ensta/ensta-yuheng.zhang/AIBOAT_test/Simulation/Simulation/RL/models/vecnormalize.pkl
